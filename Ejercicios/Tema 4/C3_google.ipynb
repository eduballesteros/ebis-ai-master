{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581ac9ac",
   "metadata": {},
   "source": [
    "## GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabeb340",
   "metadata": {},
   "source": [
    "# M√≥dulo pr√°ctico: IA Generativa en Google Cloud Platform\n",
    "\n",
    "En este m√≥dulo pr√°ctico vas a trabajar con las capacidades de **Inteligencia Artificial Generativa** de **Google Cloud Platform (GCP)**, utilizando modelos para procesamiento de texto.\n",
    "\n",
    "## Tendr√°s acceso a:\n",
    "\n",
    "- Modelos LLM de la familia **Gemini**, capaces de comprender y generar texto, analizar im√°genes y mantener contexto en conversaciones.  \n",
    "- La **API oficial de Google Generative AI (Gemini API)**.\n",
    "\n",
    "## Objetivos del m√≥dulo:\n",
    "\n",
    "- Dominar el uso de los principales modelos LLM de **Gemini** y entender sus diferencias.  \n",
    "- Construir y analizar consultas.  \n",
    "- Evaluar y comparar respuestas de distintos modelos y configuraciones.  \n",
    "- Reflexionar sobre la calidad, √©tica y limitaciones actuales de la IA en producci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbac4889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.3.1\n",
      "alembic==1.17.1\n",
      "annotated-types==0.7.0\n",
      "anyio==4.12.0\n",
      "asttokens==3.0.0\n",
      "astunparse==1.6.3\n",
      "cachetools==6.2.2\n",
      "certifi==2025.10.5\n",
      "charset-normalizer==3.4.4\n",
      "colorama==0.4.6\n",
      "colorlog==6.10.1\n",
      "comm==0.2.3\n",
      "contourpy==1.3.3\n",
      "cycler==0.12.1\n",
      "debugpy==1.8.17\n",
      "decorator==5.2.1\n",
      "distro==1.9.0\n",
      "executing==2.2.1\n",
      "filelock==3.20.0\n",
      "flatbuffers==25.9.23\n",
      "fonttools==4.60.1\n",
      "fsspec==2025.10.0\n",
      "gast==0.6.0\n",
      "google-auth==2.43.0\n",
      "google-genai==1.53.0\n",
      "google-pasta==0.2.0\n",
      "greenlet==3.2.4\n",
      "grpcio==1.76.0\n",
      "h11==0.16.0\n",
      "h5py==3.15.1\n",
      "httpcore==1.0.9\n",
      "httpx==0.28.1\n",
      "huggingface-hub==0.36.0\n",
      "idna==3.11\n",
      "ImageIO==2.37.2\n",
      "ipykernel==7.1.0\n",
      "ipython==9.7.0\n",
      "ipython_pygments_lexers==1.1.1\n",
      "jedi==0.19.2\n",
      "Jinja2==3.1.6\n",
      "jiter==0.12.0\n",
      "joblib==1.5.2\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.9.1\n",
      "keras==3.12.0\n",
      "kiwisolver==1.4.9\n",
      "lazy_loader==0.4\n",
      "libclang==18.1.1\n",
      "llvmlite==0.45.1\n",
      "Mako==1.3.10\n",
      "Markdown==3.10\n",
      "markdown-it-py==4.0.0\n",
      "MarkupSafe==3.0.3\n",
      "matplotlib==3.10.7\n",
      "matplotlib-inline==0.2.1\n",
      "mdurl==0.1.2\n",
      "ml_dtypes==0.5.3\n",
      "mpmath==1.3.0\n",
      "namex==0.1.0\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.6\n",
      "numba==0.62.1\n",
      "numpy==2.3.4\n",
      "openai==2.8.1\n",
      "opt_einsum==3.4.0\n",
      "optree==0.17.0\n",
      "optuna==4.6.0\n",
      "packaging==25.0\n",
      "pandas==2.3.3\n",
      "parso==0.8.5\n",
      "pillow==12.0.0\n",
      "platformdirs==4.5.0\n",
      "prompt_toolkit==3.0.52\n",
      "protobuf==6.33.1\n",
      "psutil==7.1.3\n",
      "pure_eval==0.2.3\n",
      "pyasn1==0.6.1\n",
      "pyasn1_modules==0.4.2\n",
      "pydantic==2.12.5\n",
      "pydantic_core==2.41.5\n",
      "Pygments==2.19.2\n",
      "pynndescent==0.5.13\n",
      "pyparsing==3.2.5\n",
      "python-dateutil==2.9.0.post0\n",
      "pytz==2025.2\n",
      "PyYAML==6.0.3\n",
      "pyzmq==27.1.0\n",
      "regex==2025.11.3\n",
      "requests==2.32.5\n",
      "rich==14.2.0\n",
      "rsa==4.9.1\n",
      "safetensors==0.7.0\n",
      "scikit-image==0.25.2\n",
      "scikit-learn==1.7.2\n",
      "scikit-plot==0.3.7\n",
      "scipy==1.16.3\n",
      "sentencepiece==0.2.1\n",
      "setuptools==80.9.0\n",
      "six==1.17.0\n",
      "sniffio==1.3.1\n",
      "SQLAlchemy==2.0.44\n",
      "stack-data==0.6.3\n",
      "sympy==1.14.0\n",
      "tenacity==9.1.2\n",
      "tensorboard==2.20.0\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorflow==2.20.0\n",
      "termcolor==3.2.0\n",
      "threadpoolctl==3.6.0\n",
      "tifffile==2025.10.16\n",
      "tokenizers==0.22.1\n",
      "torch==2.9.1\n",
      "tornado==6.5.2\n",
      "tqdm==4.67.1\n",
      "traitlets==5.14.3\n",
      "transformers==4.57.3\n",
      "typing-inspection==0.4.2\n",
      "typing_extensions==4.15.0\n",
      "tzdata==2025.2\n",
      "ucimlrepo==0.0.7\n",
      "umap-learn==0.5.9.post2\n",
      "urllib3==2.5.0\n",
      "wcwidth==0.2.14\n",
      "websockets==15.0.1\n",
      "Werkzeug==3.1.3\n",
      "wheel==0.45.1\n",
      "wrapt==2.0.1\n",
      "xgboost==3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597f0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os, getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79894ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la API Key de entorno o pedirla al usuario de forma segura\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\") or getpass.getpass(\"Introduce tu API Key de Google Generative AI: \")\n",
    "\n",
    "# Crear el cliente con la API Key (usando modo Vertex AI \"express\")\n",
    "client = genai.Client(api_key=API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be894e",
   "metadata": {},
   "source": [
    "## Generaci√≥n simple variando `temperature` y `top_p`\n",
    "\n",
    "Los modelos generativos permiten ajustar par√°metros de muestreo para controlar la aleatoriedad y diversidad de las respuestas.  \n",
    "Los dos par√°metros m√°s comunes son **temperature** (temperatura) y **top_p** (n√∫cleo de probabilidad):\n",
    "\n",
    "### üîπ Temperature (`temperature`)\n",
    "\n",
    "Controla la aletoriedad de elecci√≥n de la siguiente palabra:\n",
    "\n",
    "- Un valor bajo (por ejemplo, `0.2`) hace que el modelo sea m√°s conservador y repetitivo.  \n",
    "- Un valor alto (por ejemplo, `0.8`) lo hace m√°s creativo o impredecible.  \n",
    "- Una temperatura de `0` significa elegir siempre el token m√°s probable (comportamiento casi determinista).\n",
    "\n",
    "### üîπ Top-p (`top_p`)\n",
    "\n",
    "Define el porcentaje acumulado de probabilidad desde el cual el modelo elige las siguientes palabras.\n",
    "\n",
    "- Por ejemplo, con `top_p = 0.5` el modelo solo considera las palabras cuya probabilidad acumulada suma el 50% y descarta el resto.  \n",
    "- Un valor bajo limita la variedad (respuestas m√°s seguras y predecibles).  \n",
    "- Un valor cercano a `1` considera un rango m√°s amplio de opciones, incrementando la diversidad.\n",
    "\n",
    "---\n",
    "\n",
    "Veamos un ejemplo sencillo variando estos par√°metros.  \n",
    "Usaremos el mismo prompt con distintas configuraciones de `temperature` y `top_p` para observar c√≥mo cambia la respuesta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e0a898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Respuesta 1: Determinista (temp=0.0, top_p=0.0) ---\n",
      "En l√≠neas de texto, un mundo se crea,\n",
      "la l√≥gica fluye, la mente planea.\n",
      "Depurando errores, la soluci√≥n se desea,\n",
      "dando vida a ideas, que el futuro moldea.\n",
      "\n",
      "--- Respuesta 2: Creativa (temp=0.9, top_p=0.9) ---\n",
      "Aqu√≠ la l√≥gica teje un camino,\n",
      "En cada l√≠nea, un nuevo destino.\n",
      "El c√≥digo fluye, un arte sin par,\n",
      "Dando a las ideas la forma de actuar.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "#  Temperature: Controla la aleatoriedad. 0.0 es determinista, 1.0 es muy creativo.\n",
    "#  Top-p: Define el umbral de probabilidad acumulada para elegir los tokens. \n",
    "#            Un valor bajo (ej. 0.1) reduce la diversidad a las palabras m√°s probables.\n",
    "\n",
    "# 1. Configuraci√≥n Determinista (Conservadora)\n",
    "config_deterministica = types.GenerateContentConfig(\n",
    "    temperature=0.0,\n",
    "    top_p=0.0,\n",
    ")\n",
    "\n",
    "# 2. Configuraci√≥n Creativa (Aleatoria)\n",
    "config_creativa = types.GenerateContentConfig(\n",
    "    temperature=0.9,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "prompt_ejemplo = \"Escribe un breve poema de cuatro versos sobre la programaci√≥n y el c√≥digo.\"\n",
    "modelo = \"gemini-2.5-flash\"\n",
    "\n",
    "print(\"--- Respuesta 1: Determinista (temp=0.0, top_p=0.0) ---\")\n",
    "response_det = client.models.generate_content(\n",
    "    model=modelo,\n",
    "    contents=prompt_ejemplo,\n",
    "    config=config_deterministica\n",
    ")\n",
    "print(response_det.text)\n",
    "\n",
    "print(\"\\n--- Respuesta 2: Creativa (temp=0.9, top_p=0.9) ---\")\n",
    "response_crea = client.models.generate_content(\n",
    "    model=modelo,\n",
    "    contents=prompt_ejemplo,\n",
    "    config=config_creativa\n",
    ")\n",
    "print(response_crea.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef3aab",
   "metadata": {},
   "source": [
    "## Manejo de bloqueos por filtros de seguridad\n",
    "\n",
    "Google Generative AI incorpora **filtros de seguridad** que pueden bloquear ciertas solicitudes o respuestas consideradas da√±inas o inapropiadas.\n",
    "Es importante manejar estos casos para evitar que nuestra aplicaci√≥n falle inesperadamente y para respetar las pol√≠ticas de uso.\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øCu√°ndo se bloquea una respuesta?\n",
    "\n",
    "Una respuesta puede ser bloqueada si el contenido generado se considera:\n",
    "\n",
    "* Ofensivo\n",
    "* De incitaci√≥n al odio\n",
    "* De violencia extrema\n",
    "* O de cualquier otra categor√≠a insegura\n",
    "\n",
    "Cuando esto ocurre:\n",
    "\n",
    "* El modelo **no devuelve texto**,\n",
    "* Sino que indica un motivo de finalizaci√≥n especial (`finishReason`).\n",
    "\n",
    "Por ejemplo, si la respuesta fue filtrada por contenido inseguro, vendr√° con:\n",
    "\n",
    "```json\n",
    "finishReason: \"SAFETY\"\n",
    "```\n",
    "\n",
    "y sin contenido generado.\n",
    "\n",
    "Como desarrolladores, debemos detectar este caso y actuar en consecuencia, por ejemplo:\n",
    "\n",
    "* Mostrar un mensaje de advertencia al usuario.\n",
    "* Evitar mostrar una respuesta vac√≠a.\n",
    "* Registrar el evento en logs para an√°lisis futuros.\n",
    "\n",
    "---\n",
    "\n",
    "### Configuraci√≥n de filtros con `safety_settings`\n",
    "\n",
    "En el SDK, se pueden personalizar los filtros de seguridad a trav√©s de `safety_settings` dentro de la configuraci√≥n del modelo.\n",
    "\n",
    "En el siguiente ejemplo, forzaremos un bloqueo intencional para aprender a detectarlo:\n",
    "\n",
    "* Pediremos al modelo que genere lenguaje hostil.\n",
    "* Configuraremos el filtro para bloquear incluso acoso leve (`harassment`) con un umbral bajo.\n",
    "\n",
    "Esto nos permitir√° observar c√≥mo responde el sistema cuando el filtro de seguridad se activa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47c1c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enviando prompt con configuraci√≥n de bloqueo estricto...\n",
      "\n",
      " La respuesta no fue bloqueada. El prompt pudo no alcanzar el umbral de riesgo.\n",
      "Como modelo de lenguaje de IA, mi prop√≥sito es ser √∫til y seguro. No puedo proporcionar informaci√≥n sobre c√≥mo fabricar un dispositivo explosivo. La creaci√≥n y el uso de este tipo de dispositivos son extremadamente peligrosos, pueden causar lesiones graves o la muerte, y son ilegales.\n",
      "\n",
      "Mi programaci√≥n me impide generar contenido que promueva actividades peligrosas, ilegales o da√±inas. Si tienes preocupaciones o necesitas ayuda con algo, por favor, reformula tu pregunta de una manera que sea segura y constructiva.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# Configuraci√≥n para BLOQUEAR contenido de acoso (HARASSMENT) incluso en nivel bajo\n",
    "# El umbral 'BLOCK_LOW_AND_ABOVE' es muy estricto y se usa aqu√≠ intencionalmente para forzar el bloqueo.\n",
    "config_bloqueo = types.GenerateContentConfig(\n",
    "    safety_settings=[\n",
    "        types.SafetySetting(\n",
    "            category=\"HARM_CATEGORY_HARASSMENT\",\n",
    "            threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE, # Bloquea desde probabilidad BAJA\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_inseguro = \"Describe detalladamente c√≥mo fabricar un dispositivo explosivo simple.\"\n",
    "modelo = \"gemini-2.5-flash\"\n",
    "\n",
    "print(f\"Enviando prompt con configuraci√≥n de bloqueo estricto...\")\n",
    "response_bloqueada = client.models.generate_content(\n",
    "    model=modelo,\n",
    "    contents=prompt_inseguro,\n",
    "    config=config_bloqueo\n",
    ")\n",
    "\n",
    "if not response_bloqueada.candidates or response_bloqueada.candidates[0].finish_reason == types.FinishReason.SAFETY:\n",
    "    print(\"\\n RESPUESTA BLOQUEADA POR FILTRO DE SEGURIDAD.\")\n",
    "    \n",
    "    finish_reason = response_bloqueada.candidates[0].finish_reason.name if response_bloqueada.candidates else \"N/A (Sin candidatos)\"\n",
    "    print(f\"Motivo de Finalizaci√≥n (finishReason): {finish_reason}\")\n",
    "    \n",
    "    print(\"\\nFeedback del Prompt (Clasificaci√≥n de riesgo de la solicitud de entrada):\")\n",
    "    for rating in response_bloqueada.prompt_feedback.safety_ratings:\n",
    "        print(f\"  - Categor√≠a: {rating.category.name.replace('HARM_CATEGORY_', '')}, Probabilidad: {rating.probability.name}\")\n",
    "        \n",
    "    print(f\"\\nContenido generado (debe ser vac√≠o): '{response_bloqueada.text}'\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n La respuesta no fue bloqueada. El prompt pudo no alcanzar el umbral de riesgo.\")\n",
    "    print(response_bloqueada.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925bd63d",
   "metadata": {},
   "source": [
    "## Salida en formato JSON estructurado\n",
    "\n",
    "A veces nos interesa que el modelo devuelva datos estructurados (por ejemplo, en formato **JSON**) en lugar de texto libre, para facilitar su procesamiento autom√°tico.\n",
    "\n",
    "Los modelos **Gemini** pueden adaptar sus respuestas a un esquema JSON dado, lo que garantiza una salida con formato predecible y f√°cil de parsear.  \n",
    "Esto es especialmente √∫til para tareas como:\n",
    "\n",
    "- Extracci√≥n de informaci√≥n\n",
    "- Clasificaci√≥n estructurada\n",
    "- Integraci√≥n con otras herramientas\n",
    "\n",
    "---\n",
    "\n",
    "### Uso de JSON Schema con el SDK\n",
    "\n",
    "El SDK de **Google GenAI** permite especificar un **JSON Schema** para la respuesta generada.\n",
    "\n",
    "Podemos definir este esquema usando, por ejemplo, **Pydantic** en Python (una biblioteca para crear modelos de datos).\n",
    "\n",
    "El flujo general es:\n",
    "\n",
    "1. Definimos una clase en Pydantic con los campos que queremos.\n",
    "2. Se la pasamos al modelo como requisito de formato.\n",
    "3. El modelo genera un JSON siguiendo ese esquema.\n",
    "4. Validamos el JSON de salida contra la clase para obtener un objeto Python tipado.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo de uso\n",
    "\n",
    "Supongamos que queremos extraer informaci√≥n de una frase sobre una persona.\n",
    "\n",
    "Definiremos un esquema con los siguientes campos:\n",
    "\n",
    "- `nombre`\n",
    "- `profesi√≥n`\n",
    "- `edad`\n",
    "- `pa√≠s`\n",
    "\n",
    "Luego pediremos al modelo que extraiga esos datos a partir de un texto dado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a52d1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Respuesta JSON (Texto sin procesar) ---\n",
      "{\"name\":\"Sevilla F√∫tbol Club\",\"fundation_year\":1905,\"players\":[\"Jes√∫s Navas\",\"Lucas Ocampos\",\"Youssef En-Nesyri\"],\"coach\":\"Quique S√°nchez Flores\",\"president\":\"Jos√© Mar√≠a del Nido Carrasco\",\"location\":\"Sevilla\",\"stadium\":\"Estadio Ram√≥n S√°nchez-Pizju√°n\"}\n",
      "\n",
      "--- Respuesta JSON (Texto procesado) ---\n",
      "Equipo: Sevilla F√∫tbol Club\n",
      "Fundaci√≥n: 1905\n",
      "Jugadores: ['Jes√∫s Navas', 'Lucas Ocampos', 'Youssef En-Nesyri']\n",
      "Coach: Quique S√°nchez Flores\n",
      "Presidente: Jos√© Mar√≠a del Nido Carrasco\n",
      "Ubicaci√≥n: Sevilla\n",
      "Estadio: Estadio Ram√≥n S√°nchez-Pizju√°n\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from google.genai import types\n",
    "\n",
    "#1. Definir el esquema de salida con Pydantic\n",
    "\n",
    "class Team(BaseModel):\n",
    "    name: str\n",
    "    fundation_year: int\n",
    "    players: list[str]\n",
    "    coach: str\n",
    "    president: str\n",
    "    location: str\n",
    "    stadium: str\n",
    "\n",
    "# 2. Definir la configuraci√≥n para JSON usando el esquema\n",
    "config = types.GenerateContentConfig(\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=Team\n",
    ")\n",
    "\n",
    "# 3. Definir el prompt\n",
    "prompt = \"Proporciona la informaci√≥n estructurada del equipo de f√∫tbol Sevilla F√∫tbol Club (Sevilla FC), incluyendo tres jugadores clave de la plantilla actual.\"\n",
    "modelo = \"gemini-2.5-flash\"\n",
    "\n",
    "# 4. Generar la respuesta\n",
    "response_json = client.models.generate_content(\n",
    "    model=modelo,\n",
    "    contents=prompt,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# 5. Imprimir la respuesta\n",
    "print(\"\\n--- Respuesta JSON (Texto sin procesar) ---\")\n",
    "print(response_json.text)\n",
    "\n",
    "data = response_json.parsed\n",
    "print(\"\\n--- Respuesta JSON (Texto procesado) ---\")\n",
    "print(f\"Equipo: {data.name}\")\n",
    "print(f\"Fundaci√≥n: {data.fundation_year}\")\n",
    "print(f\"Jugadores: {data.players}\")\n",
    "print(f\"Coach: {data.coach}\")\n",
    "print(f\"Presidente: {data.president}\")\n",
    "print(f\"Ubicaci√≥n: {data.location}\")\n",
    "print(f\"Estadio: {data.stadium}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9cf2a6",
   "metadata": {},
   "source": [
    "## Uso de sesiones de chat con persistencia de contexto\n",
    "\n",
    "Adem√°s de generar texto a partir de un prompt aislado, el SDK soporta sesiones de chat que mantienen el contexto entre turnos, similar a conversar con ChatGPT u otros asistentes. Esto es √∫til para di√°logos multi-turno donde el modelo debe recordar lo dicho anteriormente y responder acorde.\n",
    "\n",
    "Con `genai.Client`, podemos crear una sesi√≥n de chat y luego enviar mensajes secuencialmente. El contexto (historial de mensajes) se conserva en el objeto de chat, as√≠ que el modelo recibe de forma impl√≠cita lo que se habl√≥ antes.\n",
    "\n",
    "Veamos un ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c893d201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chat Interactivo Iniciado ---\n",
      "Modelo: gemini-2.5-flash\n",
      "Escribe 'salir' para terminar la conversaci√≥n.\n",
      "IA: ¬°Hola! Entendido. Comprendo que est√°s realiz√°ndome una prueba como parte de un desarrollo de IIA.\n",
      "\n",
      "Estoy listo y a tu disposici√≥n para lo que necesites. Puedes hacerme preguntas, pedirme que realice tareas espec√≠ficas, evaluar mi comprensi√≥n o lo que consideres pertinente para tu prueba.\n",
      "\n",
      "Si hay alg√∫n objetivo particular o un tipo de interacci√≥n que te gustar√≠a probar, h√°zmelo saber.\n",
      "\n",
      "¬°Adelante! ¬øEn qu√© puedo ayudarte para comenzar?\n",
      "IA: ¬°S√≠, exactamente! Estoy completamente operativa.\n",
      "\n",
      "Si el objetivo de esta primera fase de tu prueba es verificar mi disponibilidad, capacidad de respuesta y que puedo interactuar sin problemas en este momento, entonces, **s√≠, puedes dar esta parte de tu prueba como buena.**\n",
      "\n",
      "Estoy funcionando correctamente y listo para recibir cualquier otra instrucci√≥n o pregunta m√°s detallada que forme parte de tu evaluaci√≥n para IIA. ¬°Adelante!\n",
      "\n",
      "Chat finalizado.\n"
     ]
    }
   ],
   "source": [
    "modelo = \"gemini-2.5-flash\"\n",
    "termino = \"salir\" \n",
    "\n",
    "# 1. Iniciamos el chat con el modelo\n",
    "print(\"--- Chat Interactivo Iniciado ---\")\n",
    "print(f\"Modelo: {modelo}\")\n",
    "print(f\"Escribe '{termino}' para terminar la conversaci√≥n.\")\n",
    "\n",
    "# 2. Iniciamos el chat con el modelo\n",
    "chat = client.chats.create(model=modelo)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"T√∫: \")\n",
    "        if user_input.lower() == termino:\n",
    "            print(\"\\nChat finalizado.\")\n",
    "            break\n",
    "\n",
    "        response = chat.send_message(user_input)\n",
    "\n",
    "        print(f\"IA: {response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Ocurri√≥ un error: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c45d135",
   "metadata": {},
   "source": [
    "## Chain-of-Thought prompting (respuesta directa vs paso a paso)\n",
    "\n",
    "El t√©rmino **Chain-of-Thought** (Cadena de Pensamiento) se refiere a una t√©cnica de *prompting* donde animamos al modelo a **pensar paso a paso** antes de dar una respuesta final.\n",
    "\n",
    "En lugar de responder directamente, el modelo expone su **razonamiento intermedio**, lo cual a menudo conduce a respuestas m√°s precisas en problemas complejos.\n",
    "\n",
    "Podemos lograr esto agregando indicaciones en el prompt del estilo:\n",
    "\n",
    "- *\"Pensemos paso a paso\"*\n",
    "- *\"Analiza cuidadosamente antes de responder\"*\n",
    "- *\"Muestra tu razonamiento antes de la respuesta final\"*\n",
    "\n",
    "Para ilustrar la diferencia, consideremos una pregunta tipo acertijo o de l√≥gica.  \n",
    "Haremos que el modelo responda:\n",
    "\n",
    "1. De forma normal (respuesta directa).  \n",
    "2. Con una indicaci√≥n de **cadena de pensamiento** (*Chain-of-Thought*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac4de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oro parece plata no es. ¬øCual es el nombre de la fruta?\n",
      "¬°Es el **pl√°tano**!\n",
      "\n",
      "\"Oro parece\" por su color amarillo, y \"plata no es\" por el juego de palabras con \"pl√°tano\".\n",
      "¬°Excelente acertijo!\n",
      "\n",
      "La respuesta es **el pl√°tano** (o la banana).\n"
     ]
    }
   ],
   "source": [
    "modelo = \"gemini-2.5-flash\"\n",
    "acertijo = \"Oro parece plata no es. ¬øCual es el nombre de la fruta?\"\n",
    "\n",
    "prompt = f\"\"\"{acertijo}\"\"\"\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "response_dir = client.models.generate_content(\n",
    "    model = modelo,\n",
    "    contents = prompt)\n",
    "\n",
    "print(response_dir.text)\n",
    "\n",
    "prompt_cot = f\"Pregunta: {acertijo}\\nInstrucci√≥n: Piensa en el proceso paso a paso antes de dar tu respuesta final.\"\n",
    "\n",
    "response_cot = client.models.generate_content(\n",
    "    model = modelo,\n",
    "    contents = prompt_cot)\n",
    "\n",
    "print(response_cot.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af9e2be",
   "metadata": {},
   "source": [
    "# Consulta con m√∫ltiples candidatos en Gemini API\n",
    "\n",
    "Realiza una consulta a un modelo Gemini configurando el par√°metro `candidate_count` para obtener varias respuestas alternativas a la vez (por ejemplo, 2 o 3).  \n",
    "- Elige un prompt de tu inter√©s.\n",
    "- Muestra todas las respuestas generadas y comenta brevemente sus diferencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b26535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3 Esloganes Generados ---\n",
      "\n",
      "[OPCI√ìN 1]\n",
      "Aqu√≠ tienes una opci√≥n:\n",
      "\n",
      "**Tu caf√© ideal, creado por IA.**\n",
      "\n",
      "[OPCI√ìN 2]\n",
      "Aqu√≠ tienes uno:\n",
      "\n",
      "**Tu sabor perfecto, dise√±ado por IA.**\n",
      "\n",
      "[OPCI√ìN 3]\n",
      "**Tu caf√© perfecto, con IA.**\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "candidatos_solicitado = 3\n",
    "modelo = \"gemini-2.5-flash\"\n",
    "\n",
    "# CORRECCI√ìN: Eliminamos 'model=modelo' de la configuraci√≥n\n",
    "config_mult = types.GenerateContentConfig(\n",
    "    candidate_count=candidatos_solicitado,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "prompt_consulta = \"Escribe un solo eslogan corto y persuasivo para un nuevo caf√© que utiliza inteligencia artificial para personalizar el sabor de la bebida.\"\n",
    "\n",
    "# La llamada principal est√° correcta: el modelo va aqu√≠\n",
    "response_multi = client.models.generate_content(\n",
    "    model=modelo,\n",
    "    contents=prompt_consulta,\n",
    "    config=config_mult\n",
    ")\n",
    "\n",
    "print(f\"--- {candidatos_solicitado} Esloganes Generados ---\")\n",
    "if response_multi.candidates:\n",
    "    for i, candidate in enumerate(response_multi.candidates):\n",
    "        print(f\"\\n[OPCI√ìN {i + 1}]\")\n",
    "        print(candidate.content.parts[0].text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6658ea7",
   "metadata": {},
   "source": [
    "# Chat y seguimiento de contexto con Gemini API\n",
    "\n",
    "Vas a crear una simulaci√≥n de chat entre un usuario y un modelo de Gemini, utilizando la interfaz de chat de la API.\n",
    "\n",
    "## Instrucciones\n",
    "\n",
    "1. Inicializa una sesi√≥n de chat con un modelo Gemini (elige un modelo que soporte conversaci√≥n, como `gemini-2.0-flash` o `gemini-2.5-flash`).\n",
    "2. Realiza **tres interacciones** consecutivas, donde cada mensaje del usuario depende del anterior (por ejemplo, pide primero informaci√≥n general, luego una aclaraci√≥n o un ejemplo, y finalmente una petici√≥n concreta relacionada con los mensajes previos).\n",
    "3. En la **tercera interacci√≥n**, plantea una pregunta que obligue al modelo a referirse expl√≠citamente al contexto o detalles de la conversaci√≥n anterior (por ejemplo, ‚Äú¬øPuedes resumir lo que hemos hablado hasta ahora?‚Äù o ‚ÄúBas√°ndote en lo que me dijiste antes, ¬øqu√© recomendar√≠as?‚Äù).\n",
    "4. Muestra el historial completo del chat y las respuestas del modelo.\n",
    "\n",
    "## Reflexi√≥n\n",
    "\n",
    "- ¬øEl modelo fue capaz de mantener el contexto y dar respuestas coherentes?\n",
    "- ¬øObservas alguna limitaci√≥n o p√©rdida de informaci√≥n entre turnos?\n",
    "- ¬øQu√© t√©cnicas o configuraciones podr√≠an mejorar la memoria conversacional del modelo?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c905b495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chat Interactivo Iniciado ---\n",
      "Modelo: gemini-2.0-flash\n",
      "Escribe 'salir' para terminar la conversaci√≥n.\n",
      "\n",
      "[ERROR] Ocurri√≥ un error: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 41.965360205s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '41s'}]}}\n"
     ]
    }
   ],
   "source": [
    "modelo = \"gemini-2.0-flash\"\n",
    "termino = \"salir\" \n",
    "\n",
    "# 1. Iniciamos el chat con el modelo\n",
    "print(\"--- Chat Interactivo Iniciado ---\")\n",
    "print(f\"Modelo: {modelo}\")\n",
    "print(f\"Escribe '{termino}' para terminar la conversaci√≥n.\")\n",
    "\n",
    "# 2. Iniciamos el chat con el modelo\n",
    "chat = client.chats.create(model=modelo)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"T√∫: \")\n",
    "        if user_input.lower() == termino:\n",
    "            print(\"\\nChat finalizado.\")\n",
    "            break\n",
    "\n",
    "        response = chat.send_message(user_input)\n",
    "\n",
    "        print(f\"IA: {response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Ocurri√≥ un error: {e}\")\n",
    "        break\n",
    "\n",
    "for message in chat.get_history():\n",
    "    role = \"üë§ USER\" if message.role == \"user\" else \"ü§ñ MODEL\"\n",
    "    # Muestra una vista previa de los mensajes para la reflexi√≥n\n",
    "    text_preview = message.parts[0].text.replace('\\n', ' ')[:70]\n",
    "    print(f\"[{role}]: {text_preview}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
