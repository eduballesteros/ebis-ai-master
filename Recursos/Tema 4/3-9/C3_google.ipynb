{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581ac9ac",
   "metadata": {},
   "source": [
    "## GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabeb340",
   "metadata": {},
   "source": [
    "# M√≥dulo pr√°ctico: IA Generativa en Google Cloud Platform\n",
    "\n",
    "En este m√≥dulo pr√°ctico vas a trabajar con las capacidades de **Inteligencia Artificial Generativa** de **Google Cloud Platform (GCP)**, utilizando modelos para procesamiento de texto.\n",
    "\n",
    "## Tendr√°s acceso a:\n",
    "\n",
    "- Modelos LLM de la familia **Gemini**, capaces de comprender y generar texto, analizar im√°genes y mantener contexto en conversaciones.  \n",
    "- La **API oficial de Google Generative AI (Gemini API)**.\n",
    "\n",
    "## Objetivos del m√≥dulo:\n",
    "\n",
    "- Dominar el uso de los principales modelos LLM de **Gemini** y entender sus diferencias.  \n",
    "- Construir y analizar consultas.  \n",
    "- Evaluar y comparar respuestas de distintos modelos y configuraciones.  \n",
    "- Reflexionar sobre la calidad, √©tica y limitaciones actuales de la IA en producci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbac4889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated-types==0.7.0\n",
      "anyio==4.11.0\n",
      "asttokens==3.0.1\n",
      "beautifulsoup4==4.14.2\n",
      "cachetools==6.2.2\n",
      "certifi==2025.11.12\n",
      "charset-normalizer==3.4.4\n",
      "colorama==0.4.6\n",
      "comm==0.2.3\n",
      "debugpy==1.8.17\n",
      "decorator==5.2.1\n",
      "executing==2.2.1\n",
      "google==3.0.0\n",
      "google-ai-generativelanguage==0.6.15\n",
      "google-api-core==2.28.1\n",
      "google-api-python-client==2.187.0\n",
      "google-auth==2.43.0\n",
      "google-auth-httplib2==0.2.1\n",
      "google-cloud==0.34.0\n",
      "google-genai==1.50.1\n",
      "google-generativeai==0.8.5\n",
      "googleapis-common-protos==1.72.0\n",
      "grpcio==1.76.0\n",
      "grpcio-status==1.71.2\n",
      "h11==0.16.0\n",
      "httpcore==1.0.9\n",
      "httplib2==0.31.0\n",
      "httpx==0.28.1\n",
      "idna==3.11\n",
      "ipykernel==7.1.0\n",
      "ipython==9.7.0\n",
      "ipython_pygments_lexers==1.1.1\n",
      "jedi==0.19.2\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.9.1\n",
      "matplotlib-inline==0.2.1\n",
      "nest-asyncio==1.6.0\n",
      "packaging==25.0\n",
      "parso==0.8.5\n",
      "platformdirs==4.5.0\n",
      "prompt_toolkit==3.0.52\n",
      "proto-plus==1.26.1\n",
      "protobuf==5.29.5\n",
      "psutil==7.1.3\n",
      "pure_eval==0.2.3\n",
      "pyasn1==0.6.1\n",
      "pyasn1_modules==0.4.2\n",
      "pydantic==2.12.4\n",
      "pydantic_core==2.41.5\n",
      "Pygments==2.19.2\n",
      "pyparsing==3.2.5\n",
      "python-dateutil==2.9.0.post0\n",
      "python-dotenv==1.2.1\n",
      "pyzmq==27.1.0\n",
      "requests==2.32.5\n",
      "rsa==4.9.1\n",
      "six==1.17.0\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.8\n",
      "stack-data==0.6.3\n",
      "tenacity==9.1.2\n",
      "tornado==6.5.2\n",
      "tqdm==4.67.1\n",
      "traitlets==5.14.3\n",
      "typing-inspection==0.4.2\n",
      "typing_extensions==4.15.0\n",
      "uritemplate==4.2.0\n",
      "urllib3==2.5.0\n",
      "wcwidth==0.2.14\n",
      "websockets==15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597f0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os, getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79894ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la API Key de entorno o pedirla al usuario de forma segura\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\") or getpass.getpass(\"Introduce tu API Key de Google Generative AI: \")\n",
    "\n",
    "# Crear el cliente con la API Key (usando modo Vertex AI \"express\")\n",
    "client = genai.Client(api_key=API_KEY, vertexai=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be894e",
   "metadata": {},
   "source": [
    "## Generaci√≥n simple variando `temperature` y `top_p`\n",
    "\n",
    "Los modelos generativos permiten ajustar par√°metros de muestreo para controlar la aleatoriedad y diversidad de las respuestas.  \n",
    "Los dos par√°metros m√°s comunes son **temperature** (temperatura) y **top_p** (n√∫cleo de probabilidad):\n",
    "\n",
    "### üîπ Temperature (`temperature`)\n",
    "\n",
    "Controla la aletoriedad de elecci√≥n de la siguiente palabra:\n",
    "\n",
    "- Un valor bajo (por ejemplo, `0.2`) hace que el modelo sea m√°s conservador y repetitivo.  \n",
    "- Un valor alto (por ejemplo, `0.8`) lo hace m√°s creativo o impredecible.  \n",
    "- Una temperatura de `0` significa elegir siempre el token m√°s probable (comportamiento casi determinista).\n",
    "\n",
    "### üîπ Top-p (`top_p`)\n",
    "\n",
    "Define el porcentaje acumulado de probabilidad desde el cual el modelo elige las siguientes palabras.\n",
    "\n",
    "- Por ejemplo, con `top_p = 0.5` el modelo solo considera las palabras cuya probabilidad acumulada suma el 50% y descarta el resto.  \n",
    "- Un valor bajo limita la variedad (respuestas m√°s seguras y predecibles).  \n",
    "- Un valor cercano a `1` considera un rango m√°s amplio de opciones, incrementando la diversidad.\n",
    "\n",
    "---\n",
    "\n",
    "Veamos un ejemplo sencillo variando estos par√°metros.  \n",
    "Usaremos el mismo prompt con distintas configuraciones de `temperature` y `top_p` para observar c√≥mo cambia la respuesta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0a898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cef3aab",
   "metadata": {},
   "source": [
    "## Manejo de bloqueos por filtros de seguridad\n",
    "\n",
    "Google Generative AI incorpora **filtros de seguridad** que pueden bloquear ciertas solicitudes o respuestas consideradas da√±inas o inapropiadas.\n",
    "Es importante manejar estos casos para evitar que nuestra aplicaci√≥n falle inesperadamente y para respetar las pol√≠ticas de uso.\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øCu√°ndo se bloquea una respuesta?\n",
    "\n",
    "Una respuesta puede ser bloqueada si el contenido generado se considera:\n",
    "\n",
    "* Ofensivo\n",
    "* De incitaci√≥n al odio\n",
    "* De violencia extrema\n",
    "* O de cualquier otra categor√≠a insegura\n",
    "\n",
    "Cuando esto ocurre:\n",
    "\n",
    "* El modelo **no devuelve texto**,\n",
    "* Sino que indica un motivo de finalizaci√≥n especial (`finishReason`).\n",
    "\n",
    "Por ejemplo, si la respuesta fue filtrada por contenido inseguro, vendr√° con:\n",
    "\n",
    "```json\n",
    "finishReason: \"SAFETY\"\n",
    "```\n",
    "\n",
    "y sin contenido generado.\n",
    "\n",
    "Como desarrolladores, debemos detectar este caso y actuar en consecuencia, por ejemplo:\n",
    "\n",
    "* Mostrar un mensaje de advertencia al usuario.\n",
    "* Evitar mostrar una respuesta vac√≠a.\n",
    "* Registrar el evento en logs para an√°lisis futuros.\n",
    "\n",
    "---\n",
    "\n",
    "### Configuraci√≥n de filtros con `safety_settings`\n",
    "\n",
    "En el SDK, se pueden personalizar los filtros de seguridad a trav√©s de `safety_settings` dentro de la configuraci√≥n del modelo.\n",
    "\n",
    "En el siguiente ejemplo, forzaremos un bloqueo intencional para aprender a detectarlo:\n",
    "\n",
    "* Pediremos al modelo que genere lenguaje hostil.\n",
    "* Configuraremos el filtro para bloquear incluso acoso leve (`harassment`) con un umbral bajo.\n",
    "\n",
    "Esto nos permitir√° observar c√≥mo responde el sistema cuando el filtro de seguridad se activa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c1c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "925bd63d",
   "metadata": {},
   "source": [
    "## Salida en formato JSON estructurado\n",
    "\n",
    "A veces nos interesa que el modelo devuelva datos estructurados (por ejemplo, en formato **JSON**) en lugar de texto libre, para facilitar su procesamiento autom√°tico.\n",
    "\n",
    "Los modelos **Gemini** pueden adaptar sus respuestas a un esquema JSON dado, lo que garantiza una salida con formato predecible y f√°cil de parsear.  \n",
    "Esto es especialmente √∫til para tareas como:\n",
    "\n",
    "- Extracci√≥n de informaci√≥n\n",
    "- Clasificaci√≥n estructurada\n",
    "- Integraci√≥n con otras herramientas\n",
    "\n",
    "---\n",
    "\n",
    "### Uso de JSON Schema con el SDK\n",
    "\n",
    "El SDK de **Google GenAI** permite especificar un **JSON Schema** para la respuesta generada.\n",
    "\n",
    "Podemos definir este esquema usando, por ejemplo, **Pydantic** en Python (una biblioteca para crear modelos de datos).\n",
    "\n",
    "El flujo general es:\n",
    "\n",
    "1. Definimos una clase en Pydantic con los campos que queremos.\n",
    "2. Se la pasamos al modelo como requisito de formato.\n",
    "3. El modelo genera un JSON siguiendo ese esquema.\n",
    "4. Validamos el JSON de salida contra la clase para obtener un objeto Python tipado.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo de uso\n",
    "\n",
    "Supongamos que queremos extraer informaci√≥n de una frase sobre una persona.\n",
    "\n",
    "Definiremos un esquema con los siguientes campos:\n",
    "\n",
    "- `nombre`\n",
    "- `profesi√≥n`\n",
    "- `edad`\n",
    "- `pa√≠s`\n",
    "\n",
    "Luego pediremos al modelo que extraiga esos datos a partir de un texto dado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52d1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e9cf2a6",
   "metadata": {},
   "source": [
    "## Uso de sesiones de chat con persistencia de contexto\n",
    "\n",
    "Adem√°s de generar texto a partir de un prompt aislado, el SDK soporta sesiones de chat que mantienen el contexto entre turnos, similar a conversar con ChatGPT u otros asistentes. Esto es √∫til para di√°logos multi-turno donde el modelo debe recordar lo dicho anteriormente y responder acorde.\n",
    "\n",
    "Con `genai.Client`, podemos crear una sesi√≥n de chat y luego enviar mensajes secuencialmente. El contexto (historial de mensajes) se conserva en el objeto de chat, as√≠ que el modelo recibe de forma impl√≠cita lo que se habl√≥ antes.\n",
    "\n",
    "Veamos un ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893d201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c45d135",
   "metadata": {},
   "source": [
    "## Chain-of-Thought prompting (respuesta directa vs paso a paso)\n",
    "\n",
    "El t√©rmino **Chain-of-Thought** (Cadena de Pensamiento) se refiere a una t√©cnica de *prompting* donde animamos al modelo a **pensar paso a paso** antes de dar una respuesta final.\n",
    "\n",
    "En lugar de responder directamente, el modelo expone su **razonamiento intermedio**, lo cual a menudo conduce a respuestas m√°s precisas en problemas complejos.\n",
    "\n",
    "Podemos lograr esto agregando indicaciones en el prompt del estilo:\n",
    "\n",
    "- *\"Pensemos paso a paso\"*\n",
    "- *\"Analiza cuidadosamente antes de responder\"*\n",
    "- *\"Muestra tu razonamiento antes de la respuesta final\"*\n",
    "\n",
    "Para ilustrar la diferencia, consideremos una pregunta tipo acertijo o de l√≥gica.  \n",
    "Haremos que el modelo responda:\n",
    "\n",
    "1. De forma normal (respuesta directa).  \n",
    "2. Con una indicaci√≥n de **cadena de pensamiento** (*Chain-of-Thought*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4de6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5af9e2be",
   "metadata": {},
   "source": [
    "# Consulta con m√∫ltiples candidatos en Gemini API\n",
    "\n",
    "Realiza una consulta a un modelo Gemini configurando el par√°metro `candidate_count` para obtener varias respuestas alternativas a la vez (por ejemplo, 2 o 3).  \n",
    "- Elige un prompt de tu inter√©s.\n",
    "- Muestra todas las respuestas generadas y comenta brevemente sus diferencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b26535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6658ea7",
   "metadata": {},
   "source": [
    "# Chat y seguimiento de contexto con Gemini API\n",
    "\n",
    "Vas a crear una simulaci√≥n de chat entre un usuario y un modelo de Gemini, utilizando la interfaz de chat de la API.\n",
    "\n",
    "## Instrucciones\n",
    "\n",
    "1. Inicializa una sesi√≥n de chat con un modelo Gemini (elige un modelo que soporte conversaci√≥n, como `gemini-2.0-flash` o `gemini-2.5-flash`).\n",
    "2. Realiza **tres interacciones** consecutivas, donde cada mensaje del usuario depende del anterior (por ejemplo, pide primero informaci√≥n general, luego una aclaraci√≥n o un ejemplo, y finalmente una petici√≥n concreta relacionada con los mensajes previos).\n",
    "3. En la **tercera interacci√≥n**, plantea una pregunta que obligue al modelo a referirse expl√≠citamente al contexto o detalles de la conversaci√≥n anterior (por ejemplo, ‚Äú¬øPuedes resumir lo que hemos hablado hasta ahora?‚Äù o ‚ÄúBas√°ndote en lo que me dijiste antes, ¬øqu√© recomendar√≠as?‚Äù).\n",
    "4. Muestra el historial completo del chat y las respuestas del modelo.\n",
    "\n",
    "## Reflexi√≥n\n",
    "\n",
    "- ¬øEl modelo fue capaz de mantener el contexto y dar respuestas coherentes?\n",
    "- ¬øObservas alguna limitaci√≥n o p√©rdida de informaci√≥n entre turnos?\n",
    "- ¬øQu√© t√©cnicas o configuraciones podr√≠an mejorar la memoria conversacional del modelo?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c905b495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
