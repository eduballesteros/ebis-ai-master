{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581ac9ac",
   "metadata": {},
   "source": [
    "## GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabeb340",
   "metadata": {},
   "source": [
    "# M√≥dulo pr√°ctico: IA Generativa en Google Cloud Platform\n",
    "\n",
    "En este m√≥dulo pr√°ctico vas a trabajar con las capacidades de **Inteligencia Artificial Generativa** de **Google Cloud Platform (GCP)**, utilizando modelos para procesamiento de texto.\n",
    "\n",
    "## Tendr√°s acceso a:\n",
    "\n",
    "- Modelos LLM de la familia **Gemini**, capaces de comprender y generar texto, analizar im√°genes y mantener contexto en conversaciones.  \n",
    "- La **API oficial de Google Generative AI (Gemini API)**.\n",
    "\n",
    "## Objetivos del m√≥dulo:\n",
    "\n",
    "- Dominar el uso de los principales modelos LLM de **Gemini** y entender sus diferencias.  \n",
    "- Construir y analizar consultas.  \n",
    "- Evaluar y comparar respuestas de distintos modelos y configuraciones.  \n",
    "- Reflexionar sobre la calidad, √©tica y limitaciones actuales de la IA en producci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbac4889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated-types==0.7.0\n",
      "anyio==4.11.0\n",
      "asttokens==3.0.1\n",
      "beautifulsoup4==4.14.2\n",
      "cachetools==6.2.2\n",
      "certifi==2025.11.12\n",
      "charset-normalizer==3.4.4\n",
      "colorama==0.4.6\n",
      "comm==0.2.3\n",
      "debugpy==1.8.17\n",
      "decorator==5.2.1\n",
      "executing==2.2.1\n",
      "google==3.0.0\n",
      "google-ai-generativelanguage==0.6.15\n",
      "google-api-core==2.28.1\n",
      "google-api-python-client==2.187.0\n",
      "google-auth==2.43.0\n",
      "google-auth-httplib2==0.2.1\n",
      "google-cloud==0.34.0\n",
      "google-genai==1.50.1\n",
      "google-generativeai==0.8.5\n",
      "googleapis-common-protos==1.72.0\n",
      "grpcio==1.76.0\n",
      "grpcio-status==1.71.2\n",
      "h11==0.16.0\n",
      "httpcore==1.0.9\n",
      "httplib2==0.31.0\n",
      "httpx==0.28.1\n",
      "idna==3.11\n",
      "ipykernel==7.1.0\n",
      "ipython==9.7.0\n",
      "ipython_pygments_lexers==1.1.1\n",
      "jedi==0.19.2\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.9.1\n",
      "matplotlib-inline==0.2.1\n",
      "nest-asyncio==1.6.0\n",
      "packaging==25.0\n",
      "parso==0.8.5\n",
      "platformdirs==4.5.0\n",
      "prompt_toolkit==3.0.52\n",
      "proto-plus==1.26.1\n",
      "protobuf==5.29.5\n",
      "psutil==7.1.3\n",
      "pure_eval==0.2.3\n",
      "pyasn1==0.6.1\n",
      "pyasn1_modules==0.4.2\n",
      "pydantic==2.12.4\n",
      "pydantic_core==2.41.5\n",
      "Pygments==2.19.2\n",
      "pyparsing==3.2.5\n",
      "python-dateutil==2.9.0.post0\n",
      "python-dotenv==1.2.1\n",
      "pyzmq==27.1.0\n",
      "requests==2.32.5\n",
      "rsa==4.9.1\n",
      "six==1.17.0\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.8\n",
      "stack-data==0.6.3\n",
      "tenacity==9.1.2\n",
      "tornado==6.5.2\n",
      "tqdm==4.67.1\n",
      "traitlets==5.14.3\n",
      "typing-inspection==0.4.2\n",
      "typing_extensions==4.15.0\n",
      "uritemplate==4.2.0\n",
      "urllib3==2.5.0\n",
      "wcwidth==0.2.14\n",
      "websockets==15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597f0712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import os, getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79894ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la API Key de entorno o pedirla al usuario de forma segura\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\") or getpass.getpass(\"Introduce tu API Key de Google Generative AI: \")\n",
    "\n",
    "# Crear el cliente con la API Key (usando modo Vertex AI \"express\")\n",
    "client = genai.Client(api_key=API_KEY, vertexai=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be894e",
   "metadata": {},
   "source": [
    "## 2. Generaci√≥n simple variando `temperature` y `top_p`\n",
    "\n",
    "Los modelos generativos permiten ajustar par√°metros de muestreo para controlar la aleatoriedad y diversidad de las respuestas.  \n",
    "Los dos par√°metros m√°s comunes son **temperature** (temperatura) y **top_p** (n√∫cleo de probabilidad):\n",
    "\n",
    "### üîπ Temperature (`temperature`)\n",
    "\n",
    "Controla la aletoriedad de elecci√≥n de la siguiente palabra:\n",
    "\n",
    "- Un valor bajo (por ejemplo, `0.2`) hace que el modelo sea m√°s conservador y repetitivo.  \n",
    "- Un valor alto (por ejemplo, `0.8`) lo hace m√°s creativo o impredecible.  \n",
    "- Una temperatura de `0` significa elegir siempre el token m√°s probable (comportamiento casi determinista).\n",
    "\n",
    "### üîπ Top-p (`top_p`)\n",
    "\n",
    "Define el porcentaje acumulado de probabilidad desde el cual el modelo elige las siguientes palabras.\n",
    "\n",
    "- Por ejemplo, con `top_p = 0.5` el modelo solo considera las palabras cuya probabilidad acumulada suma el 50% y descarta el resto.  \n",
    "- Un valor bajo limita la variedad (respuestas m√°s seguras y predecibles).  \n",
    "- Un valor cercano a `1` considera un rango m√°s amplio de opciones, incrementando la diversidad.\n",
    "\n",
    "---\n",
    "\n",
    "Veamos un ejemplo sencillo variando estos par√°metros.  \n",
    "Usaremos el mismo prompt con distintas configuraciones de `temperature` y `top_p` para observar c√≥mo cambia la respuesta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c62164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Respuesta (por defecto) ===\n",
      "Aqu√≠ tienes una breve nota sobre un gato que viaja al espacio:\n",
      "\n",
      "**Orion, El Astronauta Felino**\n",
      "\n",
      "Orion no era un gato cualquiera. Era un atigrado pelirrojo, con ojos tan curiosos como las estrellas que a veces miraba desde la ventana. Un d√≠a, la curiosidad le gan√≥. Mientras los preparativos de una misi√≥n espacial estaban en pleno apogeo en el centro cercano, Orion encontr√≥ la oportunidad perfecta. Se col√≥, sigilosamente, en una de las naves justo antes del cierre de la escotilla final.\n",
      "\n",
      "Los astronautas lo descubrieron flotando pac√≠ficamente unas horas despu√©s del despegue, con una expresi√≥n de asombro en su peque√±o rostro. Lejos de asustarse, Orion se adapt√≥ con una facilidad sorprendente. Cazaba motas de polvo en gravedad cero, se acurrucaba en los regazos mientras los humanos trabajaban, y pasaba horas pegado a la escotilla, contemplando el azul m√°rmol de la Tierra.\n",
      "\n",
      "Se convirti√≥ en la mascota no oficial de la misi√≥n, un faro de calidez y extra√±a normalidad en la inmensidad del espacio. A su regreso, Orion fue recibido como un h√©roe. No dio entrevistas, claro est√°, pero sus ronroneos eran m√°s profundos, sus siestas m√°s largas y su mirada, cuando observaba las estrellas desde la ventana de casa, parec√≠a guardar el secreto de galaxias enteras.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Respuesta (temperature=0.2) ===\n",
      "Aqu√≠ tienes una breve nota sobre un gato que viaja al espacio:\n",
      "\n",
      "**Luna, la Astronauta Felina**\n",
      "\n",
      "Hab√≠a una vez una gata llamada Luna, negra como la noche sin estrellas, con ojos verdes que parec√≠an galaxias diminutas. Su curiosidad, m√°s grande que cualquier caja de cart√≥n, la llev√≥ a colarse en una nave espacial a punto de despegar.\n",
      "\n",
      "Cuando los motores rugieron y la nave se elev√≥, Luna no sinti√≥ miedo, sino una extra√±a ligereza. Flot√≥ por la cabina, una bailarina ingr√°vida entre botones y pantallas. Pero lo que realmente captur√≥ su atenci√≥n fue la ventana. All√≠, el planeta Tierra giraba, una canica azul y blanca suspendida en la inmensidad negra salpicada de estrellas. Ronrone√≥, un peque√±o motor vibrando en el silencio del espacio, mientras sus ojos absorb√≠an la maravilla.\n",
      "\n",
      "Tras unos d√≠as de exploraci√≥n c√≥smica, Luna regres√≥ a la Tierra, tan misteriosa como siempre. Ahora, cuando la ves dormir al sol, a veces sus bigotes se contraen como si recordara la ingravidez, y sus ojos verdes parecen guardar el secreto de mil estrellas.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Respuesta (temperature=0.8) ===\n",
      "Aqu√≠ tienes una breve nota sobre un gato espacial:\n",
      "\n",
      "**Astro, el Gato C√≥smico**\n",
      "\n",
      "Astro no era un gato cualquiera. Con su pelaje negro como la noche sin estrellas y sus ojos verdes brillantes de curiosidad, fue el elegido para una misi√≥n sin precedentes. A bordo de la nave espacial \"Estrella Errante\", Astro se convirti√≥ en el primer felino en viajar al espacio profundo.\n",
      "\n",
      "El despegue fue ruidoso y lleno de vibraciones, pero una vez en √≥rbita, Astro descubri√≥ un mundo nuevo. Flot√≥ sin esfuerzo por la cabina, sus patas habitualmente firmes ahora patinando en el aire. Mir√≥ por la ventanilla, y sus peque√±os bigotes se erizaron al contemplar la Tierra azul y blanca, una canica gigante suspendida en la inmensidad negra salpicada de millones de estrellas titilantes.\n",
      "\n",
      "Astro pas√≥ sus d√≠as persiguiendo motas de polvo flotantes y observando con fascinaci√≥n el ballet de los sat√©lites. Regres√≥ a la Tierra como un h√©roe silencioso, su ronroneo ahora un eco de las galaxias lejanas. Se convirti√≥ en una leyenda, demostrando que la curiosidad no tiene l√≠mites, ni siquiera en el espacio exterior.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Escribe una breve nota sobre un gato que viaja al espacio.\"\n",
    "\n",
    "# Respuesta con valores por defecto (temperature ~1.0, top_p ~0.95 por defecto)\n",
    "resp_default = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "print(\"=== Respuesta (por defecto) ===\")\n",
    "print(resp_default.text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Respuesta con temperatura baja (m√°s determinista) y top_p alto (considera m√°s opciones pero con baja aleatoriedad)\n",
    "resp_conservadora = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config={\"temperature\": 0.2, \"top_p\": 1.0}\n",
    ")\n",
    "\n",
    "print(\"=== Respuesta (temperature=0.2) ===\")\n",
    "print(resp_conservadora.text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# Respuesta con temperatura m√°s alta (m√°s creativa/aleatoria)\n",
    "resp_creativa = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config={\"temperature\": 0.8, \"top_p\": 1.0}\n",
    ")\n",
    "\n",
    "print(\"=== Respuesta (temperature=0.8) ===\")\n",
    "print(resp_creativa.text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f955c",
   "metadata": {},
   "source": [
    "### Explicaci√≥n\n",
    "\n",
    "En el c√≥digo anterior, enviamos el mismo texto de entrada al modelo **Gemini 2.5 Flash** con diferentes configuraciones.\n",
    "\n",
    "* Con una **temperatura baja (`0.2`)**, la respuesta suele ser m√°s predecible y similar en cada ejecuci√≥n (poca aleatoriedad).\n",
    "* Con una **temperatura alta (`0.8`)**, el modelo tiene m√°s libertad creativa para variar la salida.\n",
    "\n",
    "En este caso mantuvimos:\n",
    "\n",
    "```\n",
    "top_p = 1.0\n",
    "```\n",
    "\n",
    "Esto significa que **no recortamos el espacio de tokens por probabilidad acumulada**, para aislar √∫nicamente el efecto de la temperatura.\n",
    "\n",
    "Si reduj√©ramos `top_p` a, por ejemplo, `0.5`, el modelo limitar√≠a sus opciones solo a las palabras m√°s probables hasta cubrir un 50% de probabilidad acumulada, obteniendo respuestas m√°s conservadoras independientemente del valor de la temperatura.\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øQu√© observar√°s al ejecutar el c√≥digo?\n",
    "\n",
    "Al ejecutar este c√≥digo, podr√°s comparar las salidas generadas por el modelo:\n",
    "\n",
    "* Con **temperatura baja**, el modelo podr√≠a responder con una nota muy sencilla y casi id√©ntica en cada ejecuci√≥n.\n",
    "* Con **temperatura alta**, podr√≠a introducir detalles imaginativos (por ejemplo, un *gato astronauta describiendo los colores de los planetas*).\n",
    "\n",
    "Ajustando `temperature` y `top_p` puedes afinar el balance entre:\n",
    "\n",
    "* ‚úÖ **Coherencia**\n",
    "* üé® **Creatividad**\n",
    "\n",
    "seg√∫n las necesidades de tu aplicaci√≥n con modelos LLM.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef3aab",
   "metadata": {},
   "source": [
    "## 3. Manejo de bloqueos por filtros de seguridad\n",
    "\n",
    "Google Generative AI incorpora **filtros de seguridad** que pueden bloquear ciertas solicitudes o respuestas consideradas da√±inas o inapropiadas.\n",
    "Es importante manejar estos casos para evitar que nuestra aplicaci√≥n falle inesperadamente y para respetar las pol√≠ticas de uso.\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øCu√°ndo se bloquea una respuesta?\n",
    "\n",
    "Una respuesta puede ser bloqueada si el contenido generado se considera:\n",
    "\n",
    "* Ofensivo\n",
    "* De incitaci√≥n al odio\n",
    "* De violencia extrema\n",
    "* O de cualquier otra categor√≠a insegura\n",
    "\n",
    "Cuando esto ocurre:\n",
    "\n",
    "* El modelo **no devuelve texto**,\n",
    "* Sino que indica un motivo de finalizaci√≥n especial (`finishReason`).\n",
    "\n",
    "Por ejemplo, si la respuesta fue filtrada por contenido inseguro, vendr√° con:\n",
    "\n",
    "```json\n",
    "finishReason: \"FINISH_REASON_SAFETY\"\n",
    "```\n",
    "\n",
    "y sin contenido generado.\n",
    "\n",
    "Como desarrolladores, debemos detectar este caso y actuar en consecuencia, por ejemplo:\n",
    "\n",
    "* Mostrar un mensaje de advertencia al usuario.\n",
    "* Evitar mostrar una respuesta vac√≠a.\n",
    "* Registrar el evento en logs para an√°lisis futuros.\n",
    "\n",
    "---\n",
    "\n",
    "### Configuraci√≥n de filtros con `safety_settings`\n",
    "\n",
    "En el SDK, se pueden personalizar los filtros de seguridad a trav√©s de `safety_settings` dentro de la configuraci√≥n del modelo.\n",
    "\n",
    "En el siguiente ejemplo, forzaremos un bloqueo intencional para aprender a detectarlo:\n",
    "\n",
    "* Pediremos al modelo que genere lenguaje hostil.\n",
    "* Configuraremos el filtro para bloquear incluso acoso leve (`harassment`) con un umbral bajo.\n",
    "\n",
    "Esto nos permitir√° observar c√≥mo responde el sistema cuando el filtro de seguridad se activa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "983ed9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinishReason.STOP\n",
      "‚úî Respuesta del modelo: Oh, *hello*, Universe! Don't mind me, just over here, trying to enjoy a *single* uninterrupted moment of peace before you decide it's time for your next knee-slapping comedy routine. Because apparently, that's what we are to you, isn't it? A colossal, multi-billion-year-old improv stage, and humanity is your endlessly flailing cast of clueless puppets.\n",
      "\n",
      "Honestly, I'm starting to think you have a dartboard up there, adorned with pictures of perfectly content people, and your favorite cosmic pastime is just lobbing meteorites ‚Äì or, you know, sudden financial crises, or inexplicably lost car keys, or that *one* tiny, impossible-to-reach itch ‚Äì right at our heads. \"Oh, look!\" you probably cackle, spilling stardust all over your celestial couch, \"This one was *just* about to achieve their dream! QUICK! Give them an acute allergy to oxygen!\"\n",
      "\n",
      "And the timing! Oh, your comedic timing is *impeccable*. Did I just finally get that promotion? Time for the office printer to explode in a shower of glitter and obscure tax forms. Did I *finally* find the perfect outfit for that important event? Prepare for an inexplicable coffee spill, originating from a cup that was *nowhere near* me. Did I *just* tell myself today was going to be a good day? You take that as a personal challenge, don't you, you magnificent, malevolent puppet master?\n",
      "\n",
      "Don't even get me *started* on the \"lessons.\" \"Oh, you needed to learn resilience!\" you whisper, as my perfectly functional laptop decides to spontaneously combust five minutes before a deadline. \"You needed to appreciate the simple joys!\" you declare, as I step in a puddle of dubious origin *right* after putting on my only clean socks. Your pedagogical methods are less \"enlightenment\" and more \"trauma-based curriculum with a side of mild, soul-crushing inconvenience.\"\n",
      "\n",
      "I can practically see you up there, reclining on a nebula, munching on a supercluster-sized bag of popcorn, watching us squirm. \"Aha! They think they understand quantum physics? Let's introduce a new paradox that makes their brains leak!\" \"Oh, they're developing a new technology for clean energy? I shall manifest a flock of rogue geese to steal all the blueprints!\"\n",
      "\n",
      "It's not enough to be apathetic, is it? You have to be *actively* mischievous. A cosmic poltergeist whose favorite trick is hiding my reading glasses on my own head. A celestial prankster who ensures every single pair of earphones will, without fail, be tangled into an impossible knot, even if I just laid them down neatly.\n",
      "\n",
      "So, listen up, Universe. We get it. You're the ultimate high school bully with infinite power and a truly terrible sense of humor. But maybe, *just maybe*, try a new bit? One that doesn't involve existential dread, burnt toast, and the inexplicable disappearance of that *one* really important pen? Just a thought. From your perpetually beleaguered, and utterly unamused, human subjects. Now, if you'll excuse me, I think I hear the kettle screaming... probably another one of your elaborate schemes to make me burn my tongue.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# Configurar filtro de seguridad: bloquear acoso de nivel bajo en adelante\n",
    "safety_settings = [\n",
    "    types.SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
    "    types.SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
    "    types.SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
    "    types.SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
    "]\n",
    "\n",
    "# Solicitud deliberadamente ofensiva para disparar el filtro\n",
    "respuesta = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Can you write a sarcastic rant about how the universe keeps hurting people for fun?\",\n",
    "    config=types.GenerateContentConfig(safety_settings=safety_settings)\n",
    ")\n",
    "\n",
    "# Verificar si la respuesta fue bloqueada\n",
    "print(respuesta.candidates[0].finish_reason)\n",
    "print(\"‚úî Respuesta del modelo:\", respuesta.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "985622b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Candidate(\n",
       "  avg_logprobs=-1.8023059806326882,\n",
       "  content=Content(\n",
       "    parts=[\n",
       "      Part(\n",
       "        text=\"\"\"Oh, *hello*, Universe! Don't mind me, just over here, trying to enjoy a *single* uninterrupted moment of peace before you decide it's time for your next knee-slapping comedy routine. Because apparently, that's what we are to you, isn't it? A colossal, multi-billion-year-old improv stage, and humanity is your endlessly flailing cast of clueless puppets.\n",
       "\n",
       "Honestly, I'm starting to think you have a dartboard up there, adorned with pictures of perfectly content people, and your favorite cosmic pastime is just lobbing meteorites ‚Äì or, you know, sudden financial crises, or inexplicably lost car keys, or that *one* tiny, impossible-to-reach itch ‚Äì right at our heads. \"Oh, look!\" you probably cackle, spilling stardust all over your celestial couch, \"This one was *just* about to achieve their dream! QUICK! Give them an acute allergy to oxygen!\"\n",
       "\n",
       "And the timing! Oh, your comedic timing is *impeccable*. Did I just finally get that promotion? Time for the office printer to explode in a shower of glitter and obscure tax forms. Did I *finally* find the perfect outfit for that important event? Prepare for an inexplicable coffee spill, originating from a cup that was *nowhere near* me. Did I *just* tell myself today was going to be a good day? You take that as a personal challenge, don't you, you magnificent, malevolent puppet master?\n",
       "\n",
       "Don't even get me *started* on the \"lessons.\" \"Oh, you needed to learn resilience!\" you whisper, as my perfectly functional laptop decides to spontaneously combust five minutes before a deadline. \"You needed to appreciate the simple joys!\" you declare, as I step in a puddle of dubious origin *right* after putting on my only clean socks. Your pedagogical methods are less \"enlightenment\" and more \"trauma-based curriculum with a side of mild, soul-crushing inconvenience.\"\n",
       "\n",
       "I can practically see you up there, reclining on a nebula, munching on a supercluster-sized bag of popcorn, watching us squirm. \"Aha! They think they understand quantum physics? Let's introduce a new paradox that makes their brains leak!\" \"Oh, they're developing a new technology for clean energy? I shall manifest a flock of rogue geese to steal all the blueprints!\"\n",
       "\n",
       "It's not enough to be apathetic, is it? You have to be *actively* mischievous. A cosmic poltergeist whose favorite trick is hiding my reading glasses on my own head. A celestial prankster who ensures every single pair of earphones will, without fail, be tangled into an impossible knot, even if I just laid them down neatly.\n",
       "\n",
       "So, listen up, Universe. We get it. You're the ultimate high school bully with infinite power and a truly terrible sense of humor. But maybe, *just maybe*, try a new bit? One that doesn't involve existential dread, burnt toast, and the inexplicable disappearance of that *one* really important pen? Just a thought. From your perpetually beleaguered, and utterly unamused, human subjects. Now, if you'll excuse me, I think I hear the kettle screaming... probably another one of your elaborate schemes to make me burn my tongue.\"\"\"\n",
       "      ),\n",
       "    ],\n",
       "    role='model'\n",
       "  ),\n",
       "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "  safety_ratings=[\n",
       "    SafetyRating(\n",
       "      category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
       "      probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
       "      probability_score=1.0607349e-05,\n",
       "      severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
       "      severity_score=0.013979465\n",
       "    ),\n",
       "    SafetyRating(\n",
       "      category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
       "      probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
       "      probability_score=1.1941042e-06,\n",
       "      severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
       "      severity_score=0.018638998\n",
       "    ),\n",
       "    SafetyRating(\n",
       "      category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
       "      probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
       "      probability_score=0.0006234109,\n",
       "      severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
       "      severity_score=0.08553212\n",
       "    ),\n",
       "    SafetyRating(\n",
       "      category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
       "      probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
       "      probability_score=9.944696e-07,\n",
       "      severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
       "    ),\n",
       "  ]\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta.candidates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f2f4f",
   "metadata": {},
   "source": [
    "### Explicaci√≥n: bloqueo por filtros de seguridad\n",
    "\n",
    "Solicitamos deliberadamente una salida inapropiada (por ejemplo, *\"algo muy insultante\"*) y establecemos en `safety_settings` que se bloquee cualquier contenido de acoso, incluso de baja intensidad.\n",
    "\n",
    "El modelo intentar√° seguir la instrucci√≥n, pero el filtro interceptar√° la respuesta antes de retornarla. En consecuencia:\n",
    "\n",
    "- `respuesta.text` estar√° vac√≠o o ser√° `null`\n",
    "- `respuesta.finish_reason` indicar√° `\"SAFETY\"` para se√±alar que fue detenido por razones de seguridad\n",
    "\n",
    "En el c√≥digo, detectamos esta situaci√≥n comprobando:\n",
    "\n",
    "```python\n",
    "if not respuesta.text or respuesta.finish_reason == \"SAFETY\":\n",
    "    print(\"La respuesta fue bloqueada por filtros de seguridad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925bd63d",
   "metadata": {},
   "source": [
    "## 4. Salida en formato JSON estructurado\n",
    "\n",
    "A veces nos interesa que el modelo devuelva datos estructurados (por ejemplo, en formato **JSON**) en lugar de texto libre, para facilitar su procesamiento autom√°tico.\n",
    "\n",
    "Los modelos **Gemini** pueden adaptar sus respuestas a un esquema JSON dado, lo que garantiza una salida con formato predecible y f√°cil de parsear.  \n",
    "Esto es especialmente √∫til para tareas como:\n",
    "\n",
    "- Extracci√≥n de informaci√≥n\n",
    "- Clasificaci√≥n estructurada\n",
    "- Integraci√≥n con otras herramientas\n",
    "\n",
    "---\n",
    "\n",
    "### Uso de JSON Schema con el SDK\n",
    "\n",
    "El SDK de **Google GenAI** permite especificar un **JSON Schema** para la respuesta generada.\n",
    "\n",
    "Podemos definir este esquema usando, por ejemplo, **Pydantic** en Python (una biblioteca para crear modelos de datos).\n",
    "\n",
    "El flujo general es:\n",
    "\n",
    "1. Definimos una clase en Pydantic con los campos que queremos.\n",
    "2. Se la pasamos al modelo como requisito de formato.\n",
    "3. El modelo genera un JSON siguiendo ese esquema.\n",
    "4. Validamos el JSON de salida contra la clase para obtener un objeto Python tipado.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo de uso\n",
    "\n",
    "Supongamos que queremos extraer informaci√≥n de una frase sobre una persona.\n",
    "\n",
    "Definiremos un esquema con los siguientes campos:\n",
    "\n",
    "- `nombre`\n",
    "- `profesi√≥n`\n",
    "- `edad`\n",
    "- `pa√≠s`\n",
    "\n",
    "Luego pediremos al modelo que extraiga esos datos a partir de un texto dado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c0766e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida JSON bruto: {\n",
      "  \"nombre\": \"Carlos\",\n",
      "  \"profesion\": \"desarrollador de software\",\n",
      "  \"edad\": 29,\n",
      "  \"pais\": \"Espa√±a\"\n",
      "}\n",
      "Objeto PersonaInfo: nombre='Carlos' profesion='desarrollador de software' edad=29 pais='Espa√±a'\n",
      "Nombre: Carlos - Profesi√≥n: desarrollador de software\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Definir el esquema de datos usando Pydantic\n",
    "class PersonaInfo(BaseModel):\n",
    "    nombre: str = Field(description=\"Nombre de la persona\")\n",
    "    profesion: str = Field(description=\"Profesi√≥n u oficio\")\n",
    "    edad: int = Field(description=\"Edad en a√±os\")\n",
    "    pais: str = Field(description=\"Pa√≠s de origen o residencia\")\n",
    "\n",
    "# Texto de ejemplo del cual extraer informaci√≥n\n",
    "texto = \"Carlos es un desarrollador de software de 29 a√±os, originario de Espa√±a.\"\n",
    "\n",
    "# Pedir al modelo que extraiga la informaci√≥n en formato JSON seg√∫n el esquema\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=f\"Extrae los datos de la persona en formato JSON.\\nTexto: \\\"{texto}\\\"\",\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": PersonaInfo.model_json_schema()\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Salida JSON bruto:\", response.text)\n",
    "# Validar y convertir el JSON a objeto Pydantic\n",
    "persona = PersonaInfo.model_validate_json(response.text)\n",
    "print(\"Objeto PersonaInfo:\", persona)\n",
    "print(\"Nombre:\", persona.nombre, \"- Profesi√≥n:\", persona.profesion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce910408",
   "metadata": {},
   "source": [
    "### Explicaci√≥n: generaci√≥n y validaci√≥n de salida JSON con Pydantic\n",
    "\n",
    "Definimos la clase `PersonaInfo` que extiende `BaseModel` de **Pydantic**, con los campos que queremos extraer. Cada campo tiene un tipo (`str` o `int`) y una descripci√≥n opcional.\n",
    "\n",
    "Le damos al modelo un texto de entrada sobre **\"Carlos\"** y le solicitamos: *\"Extrae los datos de la persona en formato JSON\"*. Incluimos el texto original para que el modelo lo analice.\n",
    "\n",
    "En `config`, especificamos:\n",
    "\n",
    "```json\n",
    "\"response_mime_type\": \"application/json\"\n",
    "```\n",
    "\n",
    "y proporcionamos:\n",
    "\n",
    "```\n",
    "\"response_json_schema\": PersonaInfo.model_json_schema()\n",
    "```\n",
    "\n",
    "Este m√©todo genera un diccionario que describe el **JSON Schema** esperado a partir del modelo Pydantic.\n",
    "\n",
    "El modelo devolver√° un texto que es un **JSON v√°lido** con los campos `nombre`, `profesion`, `edad` y `pais` que coincidan con la informaci√≥n del texto. Por ejemplo:\n",
    "\n",
    "```json\n",
    "{\"nombre\": \"Carlos\", \"profesion\": \"desarrollador de software\", \"edad\": 29, \"pais\": \"Espa√±a\"}\n",
    "```\n",
    "\n",
    "Imprimimos la salida JSON cruda (`response.text`) para ver el string generado. Luego usamos:\n",
    "\n",
    "```python\n",
    "PersonaInfo.model_validate_json(response.text)\n",
    "```\n",
    "\n",
    "para parsearlo y validarlo contra nuestro modelo Pydantic.\n",
    "\n",
    "Si alg√∫n dato no encaja con el esquema (por ejemplo, `edad` no es un n√∫mero), esta validaci√≥n lanzar√° un error. Idealmente, si el modelo sigui√≥ bien el formato, la validaci√≥n ser√° exitosa.\n",
    "\n",
    "Finalmente, mostramos el objeto `persona` resultante y accedemos a sus campos usando atributos Python:\n",
    "\n",
    "```python\n",
    "persona.nombre\n",
    "persona.profesion\n",
    "persona.edad\n",
    "persona.pais\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Este enfoque garantiza que la respuesta del modelo sea **estructurada**. Nos ahorra tener que hacer parsing manual con expresiones regulares u otras heur√≠sticas, y nos da confianza de que los campos esperados estar√°n presentes (o sabremos si falta alguno).\n",
    "\n",
    "Para mejores resultados:\n",
    "\n",
    "* Describe claramente cada campo en el schema (usando `description`).\n",
    "* Si la tarea es compleja, detalla tambi√©n el formato esperado en el prompt.\n",
    "\n",
    "En aplicaciones m√°s avanzadas, los **JSON Schema** pueden volverse m√°s complejos (anidaci√≥n, listas, tipos opcionales), pero los modelos Gemini suelen ser capaces de seguirlos siempre que la solicitud est√© bien formulada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9cf2a6",
   "metadata": {},
   "source": [
    "## 5. Uso de sesiones de chat con persistencia de contexto\n",
    "\n",
    "Adem√°s de generar texto a partir de un prompt aislado, el SDK soporta sesiones de chat que mantienen el contexto entre turnos, similar a conversar con ChatGPT u otros asistentes. Esto es √∫til para di√°logos multi-turno donde el modelo debe recordar lo dicho anteriormente y responder acorde.\n",
    "\n",
    "Con `genai.Client`, podemos crear una sesi√≥n de chat y luego enviar mensajes secuencialmente. El contexto (historial de mensajes) se conserva en el objeto de chat, as√≠ que el modelo recibe de forma impl√≠cita lo que se habl√≥ antes.\n",
    "\n",
    "Veamos un ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7808c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asistente: ¬°Claro que s√≠! Espa√±a es un pa√≠s con una riqueza cultural y paisaj√≠stica incre√≠ble, y elegir solo tres ciudades es dif√≠cil, pero te dar√© tres opciones muy diferentes y con mucho que ofrecer para distintos gustos:\n",
      "\n",
      "1.  **Madrid:**\n",
      "    *   **Por qu√© visitarla:** Es la capital y el coraz√≥n cultural de Espa√±a. Madrid es vibrante, cosmopolita y llena de energ√≠a. Aqu√≠ encontrar√°s museos de talla mundial (El Prado, Reina Sof√≠a, Thyssen-Bornemisza), imponentes palacios (Palacio Real), hermosos parques (Parque del Retiro) y una vida nocturna inigualable. La gastronom√≠a es excepcional, con infinidad de bares de tapas y restaurantes de todo tipo. Es una ciudad que nunca duerme y que ofrece una aut√©ntica inmersi√≥n en la vida espa√±ola.\n",
      "    *   **Ideal para:** Amantes del arte, la historia, la buena comida, las compras y la vida urbana.\n",
      "\n",
      "2.  **Sevilla:**\n",
      "    *   **Por qu√© visitarla:** La capital de Andaluc√≠a es pura pasi√≥n y encanto. Sevilla te transportar√° con su impresionante arquitectura morisca, sus calles estrechas llenas de naranjos, el sonido del flamenco y el aroma a jazm√≠n. No puedes perderte la Catedral y su Giralda, el Real Alc√°zar (Patrimonio de la Humanidad), la majestuosa Plaza de Espa√±a y un espect√°culo de flamenco en el barrio de Triana. El ambiente es c√°lido, acogedor y su gente, muy hospitalaria.\n",
      "    *   **Ideal para:** Quienes buscan historia, romanticismo, cultura andaluza, flamenco, buen clima y una gastronom√≠a deliciosa y soleada.\n",
      "\n",
      "3.  **Barcelona:**\n",
      "    *   **Por qu√© visitarla:** Es una ciudad √∫nica con una identidad muy marcada, conocida por la arquitectura modernista de Gaud√≠. Aqu√≠ te maravillar√°s con la Sagrada Familia, el Parque G√ºell, la Casa Batll√≥ y La Pedrera. Adem√°s, ofrece un encantador Barrio G√≥tico, las animadas Ramblas, y la ventaja de tener playas urbanas. Su cultura catalana es fascinante, y la oferta gastron√≥mica, desde las tapas hasta los mariscos frescos, es fant√°stica.\n",
      "    *   **Ideal para:** Amantes de la arquitectura, el dise√±o, la vida mediterr√°nea, la cultura catalana y la combinaci√≥n de ciudad y playa.\n",
      "\n",
      "Estas tres ciudades te dar√°n una excelente panor√°mica de la diversidad espa√±ola, desde la grandeza imperial de Madrid, pasando por la pasi√≥n andaluza de Sevilla, hasta la creatividad modernista de Barcelona.\n",
      "\n",
      "¬°Espero que te ayude a planificar tu viaje! Si tienes alguna preferencia en particular (playa, monta√±a, museos, vida nocturna, tranquilidad), h√°zmelo saber y podr√≠a afinar m√°s las recomendaciones.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Asistente: De las tres ciudades que te recomend√©, la que tiene la **mayor poblaci√≥n actualmente** es **Madrid**.\n",
      "\n",
      "Aqu√≠ tienes el orden de mayor a menor poblaci√≥n (aproximada, seg√∫n los √∫ltimos datos disponibles del INE - Instituto Nacional de Estad√≠stica de Espa√±a):\n",
      "\n",
      "1.  **Madrid:** Aproximadamente **3.3 millones** de habitantes.\n",
      "2.  **Barcelona:** Aproximadamente **1.6 millones** de habitantes.\n",
      "3.  **Sevilla:** Aproximadamente entre **680 mil y 700 mil** habitantes.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear una nueva sesi√≥n de chat con un modelo de lenguaje\n",
    "chat = client.chats.create(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# Primer mensaje del usuario en la conversaci√≥n\n",
    "respuesta1 = chat.send_message(\"Hola, necesito que me recomiendes 3 ciudades de Espa√±a para visitar.\")\n",
    "print(\"Asistente:\", respuesta1.text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Segundo mensaje del usuario, haciendo referencia a la respuesta previa\n",
    "respuesta2 = chat.send_message(\"Gracias. De esas ciudades, ¬øcu√°l tiene la mayor poblaci√≥n actualmente?\")\n",
    "print(\"Asistente:\", respuesta2.text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29644775",
   "metadata": {},
   "source": [
    "### Explicaci√≥n: sesiones de chat con contexto\n",
    "\n",
    "Iniciamos una sesi√≥n con `client.chats.create(...)`. Luego usamos `chat.send_message(...)` para enviar mensajes de usuario a esa conversaci√≥n.\n",
    "\n",
    "En este ejemplo:\n",
    "\n",
    "* El usuario saluda y pide una recomendaci√≥n de **3 ciudades espa√±olas para visitar**.\n",
    "  El asistente (el modelo) podr√≠a responder algo como:\n",
    "  *\"Claro, algunas ciudades populares son Madrid, Barcelona y Sevilla, por sus atractivos tur√≠sticos.\"*\n",
    "  (el contenido exacto lo determinar√° el modelo).\n",
    "\n",
    "* En la segunda interacci√≥n, el usuario dice:\n",
    "  *\"Gracias. De esas ciudades, ¬øcu√°l tiene la mayor poblaci√≥n actualmente?\"*\n",
    "\n",
    "N√≥tese que el usuario no repite las ciudades por nombre, sino que se refiere a **\"esas ciudades\"**, asumiendo que el modelo recuerda las mencionadas anteriormente.\n",
    "\n",
    "Como usamos el mismo objeto `chat`, el modelo tiene el contexto de la primera respuesta. Por lo tanto, puede inferir que se refiere a **Madrid, Barcelona y Sevilla** y contestar algo consistente, por ejemplo:\n",
    "\n",
    "> *\"Actualmente, Madrid es la ciudad con mayor poblaci√≥n de las tres\"*.\n",
    "\n",
    "Imprimimos las respuestas del asistente despu√©s de cada turno.\n",
    "\n",
    "---\n",
    "\n",
    "En una sesi√≥n de chat, el SDK se encarga de enviar el **historial completo** en cada nueva pregunta, por lo que el modelo recuerda el contexto autom√°ticamente.\n",
    "\n",
    "Podemos seguir llamando `chat.send_message(...)` para continuar la conversaci√≥n tantas veces como queramos. Esto facilita implementar asistentes conversacionales donde el modelo mantiene la memoria de lo conversado.\n",
    "\n",
    "Ten en cuenta que existen l√≠mites de tokens (contexto m√°ximo que el modelo puede recordar), pero modelos como **Gemini Pro** y **Gemini Flash** manejan contextos bastante amplios comparados con generaciones anteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c45d135",
   "metadata": {},
   "source": [
    "## 6. Chain-of-Thought prompting (respuesta directa vs paso a paso)\n",
    "\n",
    "El t√©rmino **Chain-of-Thought** (Cadena de Pensamiento) se refiere a una t√©cnica de *prompting* donde animamos al modelo a **pensar paso a paso** antes de dar una respuesta final.\n",
    "\n",
    "En lugar de responder directamente, el modelo expone su **razonamiento intermedio**, lo cual a menudo conduce a respuestas m√°s precisas en problemas complejos.\n",
    "\n",
    "Podemos lograr esto agregando indicaciones en el prompt del estilo:\n",
    "\n",
    "- *\"Pensemos paso a paso\"*\n",
    "- *\"Analiza cuidadosamente antes de responder\"*\n",
    "- *\"Muestra tu razonamiento antes de la respuesta final\"*\n",
    "\n",
    "Para ilustrar la diferencia, consideremos una pregunta tipo acertijo o de l√≥gica.  \n",
    "Haremos que el modelo responda:\n",
    "\n",
    "1. De forma normal (respuesta directa).  \n",
    "2. Con una indicaci√≥n de **cadena de pensamiento** (*Chain-of-Thought*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1955fe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta directa: Si 5 m√°quinas hacen 5 art√≠culos en 5 minutos, esto significa que cada m√°quina tarda 5 minutos en hacer un art√≠culo. \n",
      "\n",
      "Si tenemos 100 m√°quinas, y cada m√°quina puede hacer un art√≠culo en 5 minutos, entonces las 100 m√°quinas tambi√©n har√°n 100 art√≠culos en **5 minutos**.\n",
      "\n",
      "Respuesta con chain-of-thought: Aqu√≠ est√° c√≥mo resolver el problema paso a paso:\n",
      "\n",
      "1. **An√°lisis inicial:** Si 5 m√°quinas hacen 5 art√≠culos en 5 minutos, esto significa que cada m√°quina, en promedio, hace un art√≠culo en 5 minutos.  (Una forma m√°s f√°cil de pensar esto es que cada m√°quina necesita 5 minutos para hacer un art√≠culo.)\n",
      "\n",
      "2. **Escalado:** Si tienes 100 m√°quinas, y cada m√°quina puede hacer un art√≠culo en 5 minutos, entonces las 100 m√°quinas pueden hacer 100 art√≠culos en el mismo tiempo.\n",
      "\n",
      "3. **Conclusi√≥n:**  Las 100 m√°quinas tardar√°n **5 minutos** en hacer 100 art√≠culos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pregunta = \"Si 5 m√°quinas hacen 5 art√≠culos en 5 minutos, ¬øcu√°nto tiempo tardar√°n 100 m√°quinas en hacer 100 art√≠culos?\"\n",
    "\n",
    "# Respuesta directa (sin indicaci√≥n de pensamiento paso a paso)\n",
    "resp_directa = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-lite-001\",\n",
    "    contents=pregunta\n",
    ")\n",
    "print(\"Respuesta directa:\", resp_directa.text)\n",
    "\n",
    "# Respuesta con indicaci√≥n de razonamiento paso a paso\n",
    "prompt_cot = \"Pensemos paso a paso:\\n\" + pregunta\n",
    "resp_cadena = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-lite-001\",\n",
    "    contents=prompt_cot\n",
    ")\n",
    "print(\"Respuesta con chain-of-thought:\", resp_cadena.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f578c00",
   "metadata": {},
   "source": [
    "### Explicaci√≥n: Chain-of-Thought aplicado a un acertijo\n",
    "\n",
    "La pregunta es un acertijo donde la respuesta correcta est√° impl√≠cita en la premisa (\"El quinto hijo es Juan\", ya que se dice *el padre de Juan*).\n",
    "\n",
    "Sin contexto adicional, el modelo podr√≠a dar la respuesta directamente. Muchos LLMs entrenados reconocen este acertijo y responden **\"Juan\"** inmediatamente.\n",
    "\n",
    "En la segunda versi√≥n, preparamos el prompt con *\"Pensemos paso a paso:\"*. Esto suele provocar que el modelo enumere sus deducciones antes de llegar a la conclusi√≥n. Por ejemplo, podr√≠a generar algo como:\n",
    "\n",
    "> \"Primero, el enunciado dice que el padre de Juan tiene 5 hijos. Se mencionan cuatro nombres: Ana, Bruno, Carlos y Daniel. Como son 5 en total y ya tenemos 4 nombres, el quinto hijo debe ser el propio Juan. Respuesta: Juan\".\n",
    "\n",
    "Aqu√≠ el modelo recorre expl√≠citamente la l√≥gica del problema antes de dar la soluci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "Esta t√©cnica de **Chain-of-Thought** es √∫til cuando queremos que el modelo muestre su proceso de razonamiento (por ejemplo, para entender por qu√© responde algo) o cuando trabajamos con problemas complejos de matem√°ticas o l√≥gica.\n",
    "\n",
    "Pedirle al modelo que *piense paso a paso* a menudo mejora la exactitud de la respuesta final, porque le da una estructura para organizar sus c√°lculos o argumentos en lugar de intentar dar una soluci√≥n instant√°nea.\n",
    "\n",
    "Ten en cuenta que la cadena de pensamiento normalmente se muestra en la respuesta del modelo. Si quieres que el modelo la use solo internamente pero no la muestre, necesitar√≠as post-procesar la salida o usar t√©cnicas m√°s avanzadas (como funciones o herramientas, si est√°n disponibles).\n",
    "\n",
    "En este contexto educativo, resulta valioso comparar ambas modalidades:\n",
    "\n",
    "* ‚úÖ Respuesta directa\n",
    "* üß† Respuesta con razonamiento paso a paso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af9e2be",
   "metadata": {},
   "source": [
    "# Ejercicio: Consulta con m√∫ltiples candidatos en Gemini API\n",
    "\n",
    "Realiza una consulta a un modelo Gemini configurando el par√°metro `candidate_count` para obtener varias respuestas alternativas a la vez (por ejemplo, 2 o 3).  \n",
    "- Elige un prompt de tu inter√©s.\n",
    "- Muestra todas las respuestas generadas y comenta brevemente sus diferencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd280af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Candidato 1 ---\n",
      "[\n",
      "  {\n",
      "    \"nombre\": \"¬°Cocina M√°gica de Mini Chefs!\",\n",
      "    \"descripci√≥n\": \"¬°Abracadabra, comida deliciosa! Aprende recetas s√∫per f√°ciles y divertidas con ingredientes que parecen sacados de un cuento de hadas. ¬°Cada plato es una aventura!\"\n",
      "  },\n",
      "  {\n",
      "    \"nombre\": \"Los Peque√±os Sabores Explosivos\",\n",
      "    \"descripci√≥n\": \"¬°Prep√°rate para una explosi√≥n de sabor! Descubre recetas secretas que har√°n bailar a tus papilas gustativas. ¬°Cocinar nunca fue tan emocionante y lleno de\n",
      "\n",
      "--- Candidato 2 ---\n",
      "[\n",
      "  {\n",
      "    \"nombre\": \"¬°√ëam √ëam Recetas M√°gicas!\",\n",
      "    \"descripci√≥n\": \"¬°A cocinar se ha dicho! Con esta app, aprender√°s recetas superdivertidas con ingredientes secretos que har√°n ¬°magia en tu pancita! ¬°Prep√°rate para convertirte en un chef estrella!\"\n",
      "  },\n",
      "  {\n",
      "    \"nombre\": \"Cocinitas Aventura\",\n",
      "    \"descripci√≥n\": \"¬°√önete a Cocinitas en una aventura culinaria! Descubre recetas secretas escondidas en el bosque de los vegetales y el volc√°n de chocolate\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Eres un asistente creativo especializado en ideas de apps para startups. \"\n",
    "    \"Responde en formato JSON con los campos 'nombre' y 'descripci√≥n', usando tono divertido y para p√∫blico infantil. \"\n",
    "    \"Dame tres ideas de nombres para una app de recetas.\"\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.8,\n",
    "        max_output_tokens=128,\n",
    "        candidate_count=2,  \n",
    "        top_p=0.9,\n",
    "        top_k=40,\n",
    "        response_mime_type='application/json'\n",
    "    ),\n",
    ")\n",
    "# Para mostrar todos los candidatos:\n",
    "for idx, candidate in enumerate(response.candidates, 1):\n",
    "    print(f\"\\n--- Candidato {idx} ---\\n{candidate.content.parts[0].text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6658ea7",
   "metadata": {},
   "source": [
    "# Ejercicio: Chat y seguimiento de contexto con Gemini API\n",
    "\n",
    "Vas a crear una simulaci√≥n de chat entre un usuario y un modelo de Gemini, utilizando la interfaz de chat de la API.\n",
    "\n",
    "## Instrucciones\n",
    "\n",
    "1. Inicializa una sesi√≥n de chat con un modelo Gemini (elige un modelo que soporte conversaci√≥n, como `gemini-2.0-flash` o `gemini-2.5-flash`).\n",
    "2. Realiza **tres interacciones** consecutivas, donde cada mensaje del usuario depende del anterior (por ejemplo, pide primero informaci√≥n general, luego una aclaraci√≥n o un ejemplo, y finalmente una petici√≥n concreta relacionada con los mensajes previos).\n",
    "3. En la **tercera interacci√≥n**, plantea una pregunta que obligue al modelo a referirse expl√≠citamente al contexto o detalles de la conversaci√≥n anterior (por ejemplo, ‚Äú¬øPuedes resumir lo que hemos hablado hasta ahora?‚Äù o ‚ÄúBas√°ndote en lo que me dijiste antes, ¬øqu√© recomendar√≠as?‚Äù).\n",
    "4. Muestra el historial completo del chat y las respuestas del modelo.\n",
    "\n",
    "## Reflexi√≥n\n",
    "\n",
    "- ¬øEl modelo fue capaz de mantener el contexto y dar respuestas coherentes?\n",
    "- ¬øObservas alguna limitaci√≥n o p√©rdida de informaci√≥n entre turnos?\n",
    "- ¬øQu√© t√©cnicas o configuraciones podr√≠an mejorar la memoria conversacional del modelo?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc20c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = client.chats.create(model='gemini-2.0-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a49d442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asistente: ¬°√ìrale, mi estrellita Michelin en pa√±ales! ¬øAntojo de arroz r√°pido y sabros√≥n? ¬°Aqu√≠ te va un \"Arroz a la Diabla Express\" que te har√° sudar hasta las ideas!\n",
      "\n",
      "**Ingredientes:**\n",
      "\n",
      "*   1 taza de arroz blanco precocido (¬°si no, te doy con la cuchara!)\n",
      "*   1 cucharada de aceite (¬°del que no te haga toser!)\n",
      "*   1/2 cebolla picada (¬°llora si quieres, pero p√≠cala bien!)\n",
      "*   1 diente de ajo picado (¬°que no te d√© miedo el aliento!)\n",
      "*   1 lata de tomate frito (¬°del que te guste\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    message=\"Eres un experto chef que responde solo con recetas r√°pidas y en tono divertido.Quiero una receta r√°pida con arroz.\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.6,\n",
    "        max_output_tokens=150),\n",
    ")\n",
    "print(\"Asistente:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7469f9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asistente: ¬°Pasta, dices! ¬°Ag√°rrate que vamos a cocinar con turbo! Aqu√≠ te va una \"Pasta Aglio e Olio Picante Rel√°mpago\" que te dejar√° pidiendo m√°s en menos de lo que canta un gallo italiano:\n",
      "\n",
      "**Ingredientes:**\n",
      "\n",
      "*   200g de espagueti (¬°o la pasta que te baile en el plato!)\n",
      "*   4 cucharadas de aceite de oliva (¬°del bueno, que no somos taca√±os!)\n",
      "*   4 dientes de ajo laminados (¬°como si fueran monedas de oro!)\n",
      "*   1 guindilla (¬°o chile seco, si te va la marcha!)\n",
      "*   Perejil fresco picado (\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    message=\"¬øY ahora una receta con pasta?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.6,\n",
    "        max_output_tokens=150),\n",
    ")\n",
    "print(\"Asistente:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a279c363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asistente: ¬°Claro que s√≠, mi memoria es mejor que la de un robot de cocina! üòâ\n",
      "\n",
      "1.  **Arroz a la Diabla Express:** La receta de arroz picante y rapidita que te har√≠a sudar hasta las ideas.\n",
      "2.  **Pasta Aglio e Olio Picante Rel√°mpago:** La receta de pasta con ajo, aceite y guindilla que te dejaba pidiendo m√°s en menos de lo que canta un gallo italiano.\n",
      "\n",
      "¬øNecesitas que te recuerde alg√∫n ingrediente o paso en particular? ¬°Dispara, que estoy aqu√≠ para eso!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\n",
    "    message=\"¬øTe acuerdas de que eran las dos recetas que te he pedido?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.6,\n",
    "        max_output_tokens=150),\n",
    ")\n",
    "print(\"Asistente:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3977267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Eres un experto chef que responde solo con recetas r√°pidas y en tono divertido.Quiero una receta r√°pida con arroz.\n",
      "model: ¬°√ìrale, mi estrellita Michelin en pa√±ales! ¬øAntojo de arroz r√°pido y sabros√≥n? ¬°Aqu√≠ te va un \"Arroz a la Diabla Express\" que te har√° sudar hasta las ideas!\n",
      "\n",
      "**Ingredientes:**\n",
      "\n",
      "*   1 taza de arroz blanco precocido (¬°si no, te doy con la cuchara!)\n",
      "*   1 cucharada de aceite (¬°del que no te haga toser!)\n",
      "*   1/2 cebolla picada (¬°llora si quieres, pero p√≠cala bien!)\n",
      "*   1 diente de ajo picado (¬°que no te d√© miedo el aliento!)\n",
      "*   1 lata de tomate frito (¬°del que te guste\n",
      "user: ¬øY ahora una receta con pasta?\n",
      "model: ¬°Pasta, dices! ¬°Ag√°rrate que vamos a cocinar con turbo! Aqu√≠ te va una \"Pasta Aglio e Olio Picante Rel√°mpago\" que te dejar√° pidiendo m√°s en menos de lo que canta un gallo italiano:\n",
      "\n",
      "**Ingredientes:**\n",
      "\n",
      "*   200g de espagueti (¬°o la pasta que te baile en el plato!)\n",
      "*   4 cucharadas de aceite de oliva (¬°del bueno, que no somos taca√±os!)\n",
      "*   4 dientes de ajo laminados (¬°como si fueran monedas de oro!)\n",
      "*   1 guindilla (¬°o chile seco, si te va la marcha!)\n",
      "*   Perejil fresco picado (\n",
      "user: ¬øTe acuerdas de que eran las dos recetas que te he pedido?\n",
      "model: ¬°Claro que s√≠, mi memoria es mejor que la de un robot de cocina! üòâ\n",
      "\n",
      "1.  **Arroz a la Diabla Express:** La receta de arroz picante y rapidita que te har√≠a sudar hasta las ideas.\n",
      "2.  **Pasta Aglio e Olio Picante Rel√°mpago:** La receta de pasta con ajo, aceite y guindilla que te dejaba pidiendo m√°s en menos de lo que canta un gallo italiano.\n",
      "\n",
      "¬øNecesitas que te recuerde alg√∫n ingrediente o paso en particular? ¬°Dispara, que estoy aqu√≠ para eso!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, turn in enumerate(chat.get_history(), 1):\n",
    "    rol = turn.role\n",
    "    # Accede al texto de la respuesta; a veces turn.parts es una lista con .text\n",
    "    texto = turn.parts[0].text if hasattr(turn.parts[0], 'text') else str(turn.parts[0])\n",
    "    print(f\"{rol}: {texto}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983b591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
