{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40c4485",
   "metadata": {},
   "source": [
    "# Autoregresión con RNN (“Mini-GPT”) en PyTorch\n",
    "\n",
    "Este notebook ilustra un modelo autoregresivo básico: toma un contexto de n palabras y predice la siguiente, repitiendo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a9ac7",
   "metadata": {},
   "source": [
    "## 1. Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets tokenizers --quiet\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123bcb9",
   "metadata": {},
   "source": [
    "## 2. Preparación del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "text = requests.get(\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\").text.lower()\n",
    "words = text.split()\n",
    "from collections import Counter\n",
    "vocab_list = [w for w,_ in Counter(words).most_common(5000)]\n",
    "vocab = {w:i for i,w in enumerate(vocab_list)}\n",
    "unk = len(vocab_list)\n",
    "data = [vocab.get(w, unk) for w in words]\n",
    "print(f\"Corpus length: {len(words)} words, Vocab size: {len(vocab_list)+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "# Load WikiText dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "texts = dataset[\"train\"][\"text\"]\n",
    "\n",
    "# Process and tokenize text\n",
    "words = []\n",
    "for text in texts:\n",
    "    if text.strip():  # Skip empty lines\n",
    "        words.extend(text.lower().split())\n",
    "\n",
    "# Create vocabulary\n",
    "vocab_list = [w for w,_ in Counter(words).most_common(8000)]  # Increased vocab size\n",
    "vocab = {w:i for i,w in enumerate(vocab_list)}\n",
    "unk = len(vocab_list)\n",
    "data = [vocab.get(w, unk) for w in words]\n",
    "print(f\"Corpus length: {len(words)} words, Vocab size: {len(vocab_list)+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d190a15",
   "metadata": {},
   "source": [
    "## 3. Dataset y DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NextWordDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=15):  # Increased sequence length\n",
    "        self.data, self.seq_len = data, seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx : idx+self.seq_len])\n",
    "        y = torch.tensor(self.data[idx+1 : idx+self.seq_len+1])\n",
    "        return x, y\n",
    "\n",
    "ds = NextWordDataset(data, seq_len=15)\n",
    "dl = DataLoader(ds, batch_size=128, shuffle=True)  # Increased batch size\n",
    "print(f\"Dataset size: {len(ds)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06fecf",
   "metadata": {},
   "source": [
    "## 4. Definición del modelo RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d42a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MiniRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hid_dim=256): # Aumentadas de 32,64 a 128,256\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, emb_dim)\n",
    "        self.rnn = nn.RNN(emb_dim, hid_dim, num_layers=2) # Añadir más capas\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size+1)\n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)           # (B, L, E)\n",
    "        out, _ = self.rnn(e)      # (B, L, H)\n",
    "        return self.fc(out)       # (B, L, V)\n",
    "\n",
    "print(MiniRNN(len(vocab_list), 32, 64))\n",
    "\n",
    "# 1. Usar LSTM en lugar de RNN simple\n",
    "class MiniLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, hid_dim=512):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, \n",
    "                           num_layers=3, \n",
    "                           dropout=0.2)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size+1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)\n",
    "        out, _ = self.lstm(e)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39cee4",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento rápido (1–2 épocas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ddbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MiniLSTM(len(vocab_list), emb_dim=256, hid_dim=512).to(device)  # Larger model\n",
    "opt = optim.Adam(model.parameters(), lr=3e-4)  # Adjusted learning rate\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20  # More epochs for larger dataset\n",
    "best_loss = float('inf')\n",
    "patience = 2\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for x, y in dl:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        opt.zero_grad()\n",
    "        loss.backward() \n",
    "        opt.step()\n",
    "        total += loss.item()\n",
    "    \n",
    "    avg_loss = total/len(dl)\n",
    "    print(f\"Época {epoch+1}, pérdida: {avg_loss:.3f}\")\n",
    "    \n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf14e70",
   "metadata": {},
   "source": [
    "## 6. Función de generación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2626b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, seed_words, length=20):\n",
    "    model.eval()\n",
    "    idxs = [vocab.get(w, unk) for w in seed_words.split()]\n",
    "    ctx  = torch.tensor(idxs[-5:])[None].to(device)\n",
    "    out_words = seed_words.split()\n",
    "    for _ in range(length):\n",
    "        logits = model(ctx)\n",
    "        next_id = logits[0, -1].argmax().item()\n",
    "        out_words.append(vocab_list[next_id] if next_id < len(vocab_list) else \"<unk>\")\n",
    "        ctx = torch.tensor([ [vocab.get(w, unk) for w in out_words[-5:]] ]).to(device)\n",
    "    return \" \".join(out_words)\n",
    "\n",
    "print(generate(model, \"to be or not to be\", length=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c243d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, seed_words, length=30, temperature=0.8):  # Increased length\n",
    "    model.eval()\n",
    "    idxs = [vocab.get(w, unk) for w in seed_words.split()]\n",
    "    ctx = torch.tensor(idxs[-15:])[None].to(device)  # Increased context window\n",
    "    out_words = seed_words.split()\n",
    "    \n",
    "    for _ in range(length):\n",
    "        logits = model(ctx)\n",
    "        probs = torch.softmax(logits[0, -1] / temperature, dim=0)\n",
    "        next_id = torch.multinomial(probs, 1).item()\n",
    "        out_words.append(vocab_list[next_id] if next_id < len(vocab_list) else \"<unk>\")\n",
    "        ctx = torch.tensor([[vocab.get(w, unk) for w in out_words[-15:]]]).to(device)\n",
    "    \n",
    "    return \" \".join(out_words)\n",
    "\n",
    "# Test different temperatures and prompts\n",
    "print(\"T=0.7:\", generate(model, \"the government announced that\", temperature=0.7))\n",
    "print(\"T=1.0:\", generate(model, \"in the middle of\", temperature=1.0))\n",
    "print(\"T=1.2:\", generate(model, \"scientists have discovered\", temperature=1.2))\n",
    "print(\"T=1.2:\", generate(model, \"to be or not to be\", temperature=1.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3b5d4",
   "metadata": {},
   "source": [
    "## 7. Experimenta en clase\n",
    "\n",
    "1. Cambia `seq_len` en el dataset (por ejemplo 3 o 10) y observa generación.\n",
    "2. Prueba muestreo en lugar de `argmax` (sample sobre softmax).\n",
    "3. Ajusta tasas de aprendizaje o emb_dim/hid_dim y comenta diferencias breves."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
